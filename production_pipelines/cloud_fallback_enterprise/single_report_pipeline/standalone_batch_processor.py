#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆå¯è¿è¡Œçš„æ‰¹é‡å¤„ç†å™¨æ¨¡å—
å¤„ç†å¤šä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶çš„å®Œæ•´ç³»ç»Ÿ
"""

import json
import ollama
from typing import Dict, List, Any
from pathlib import Path
from datetime import datetime
import time
import statistics
import re
import logging
import pickle
import os
import sys

# å¯¼å…¥å…¶ä»–æ¨¡å—
from context_generator import ContextGenerator
from reverse_scoring_processor import ReverseScoringProcessor
from input_parser import InputParser


class StandaloneBatchProcessor:
    """ç‹¬ç«‹æ‰¹é‡å¤„ç†å™¨ - ä¸éœ€è¦ç›¸å¯¹å¯¼å…¥"""
    
    def __init__(self, primary_models: List[str] = None, dispute_models: List[str] = None):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            primary_models: ä¸»è¦è¯„ä¼°æ¨¡å‹åˆ—è¡¨
            dispute_models: äº‰è®®è§£å†³æ¨¡å‹åˆ—è¡¨
        """
        self.primary_models = primary_models or [
            'qwen3:8b',
            'deepseek-r1:8b', 
            'mistral-nemo:latest'
        ]
        
        # å‡†å¤‡7ä¸ªè¯„ä¼°å™¨æ¨¡å‹ï¼Œç¡®ä¿å“ç‰Œå·®å¼‚å’Œå°ºåº¦è¦æ±‚
        self.dispute_models = dispute_models or [
            'llama3:latest',      # Meta (ç¬¬1è½®ç¬¬1ä¸ª)
            'gemma3:latest',      # Google (ç¬¬1è½®ç¬¬2ä¸ª)
            'phi3:mini',          # Microsoft (ç¬¬2è½®ç¬¬1ä¸ª)
            'yi:6b',              # 01.AI (ç¬¬2è½®ç¬¬2ä¸ª)
            'qwen3:4b',           # Alibaba (ç¬¬3è½®ç¬¬1ä¸ª)
            'deepseek-r1:8b',     # DeepSeek (ç¬¬3è½®ç¬¬2ä¸ª)
            'mixtral:8x7b'        # Mistral AI (å¤‡ç”¨)
        ]
        
        self.context_generator = ContextGenerator()
        self.reverse_processor = ReverseScoringProcessor()
        self.input_parser = InputParser()
        
        # äº‰è®®è§£å†³å‚æ•°
        self.max_dispute_rounds = 3      # æœ€å¤§äº‰è®®è§£å†³è½®æ¬¡
        self.dispute_threshold = 1.0     # äº‰è®®æ£€æµ‹é˜ˆå€¼
        self.checkpoint_interval = 5     # æ£€æŸ¥ç‚¹é—´éš”
    
    def parse_scores_from_response(self, response: str) -> Dict[str, int]:
        """
        ä»æ¨¡å‹å“åº”ä¸­è§£æè¯„åˆ†
        
        Args:
            response: æ¨¡å‹çš„å®Œæ•´å“åº”å­—ç¬¦ä¸²
            
        Returns:
            è§£æå‡ºçš„è¯„åˆ†å­—å…¸
        """
        # é»˜è®¤è¯„åˆ†
        default_scores = {
            'openness_to_experience': 3,
            'conscientiousness': 3,
            'extraversion': 3,
            'agreeableness': 3,
            'neuroticism': 3
        }
        
        if not response or not response.strip():
            print("    ä½¿ç”¨é»˜è®¤è¯„åˆ†: æ— å“åº”")
            return default_scores
        
        try:
            # å°è¯•åŒ¹é… ```json``` åŒ…è£¹çš„å†…å®¹
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                data = json.loads(json_str)
            else:
                # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                # å…ˆæŸ¥æ‰¾scoreséƒ¨åˆ†
                scores_match = re.search(r'"scores"\s*:\s*\{([^}]*)\}', response, re.DOTALL)
                if scores_match:
                    # æå–scoreséƒ¨åˆ†å¹¶æ„å»ºå®Œæ•´çš„JSON
                    scores_part = scores_match.group(0)
                    # å°è¯•è¡¥å…¨JSONç»“æ„
                    full_response = f'{{ {scores_part} }}'
                    data = json.loads(full_response)
                else:
                    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                    data = json.loads(response)
            
            if 'scores' in data:
                scores = data['scores']
                
                # ç¡®ä¿æ‰€æœ‰è¯„åˆ†éƒ½åœ¨1,3,5èŒƒå›´å†…
                for trait, score in scores.items():
                    if isinstance(score, (int, float)):
                        if score <= 1.5:
                            scores[trait] = 1
                        elif score >= 4.5:
                            scores[trait] = 5
                        else:
                            scores[trait] = 3
                    elif score in [1, 3, 5]:
                        scores[trait] = score
                    else:
                        scores[trait] = 3  # é»˜è®¤ä¸­æ€§åˆ†
                
                print(f"    è§£æè¯„åˆ†: {scores}")
                return scores
            else:
                print("    å“åº”ä¸­æœªæ‰¾åˆ°scoreså­—æ®µ")
                return default_scores
                
        except json.JSONDecodeError as e:
            print(f"    JSONè§£æå¤±è´¥: {e}")
            print(f"    å“åº”å†…å®¹é¢„è§ˆ: {response[:200]}...")
            
            # ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†
            scores = default_scores.copy()
            for trait in scores:
                # æŸ¥æ‰¾ç±»ä¼¼"trait: 3"æˆ–"trait = 5"çš„æ¨¡å¼
                pattern = rf'{trait}.*?[=:]\s*([1-5])'
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    try:
                        scores[trait] = int(match.group(1))
                    except:
                        pass  # ä¿æŒé»˜è®¤åˆ†
            
            return scores
        except Exception as e:
            print(f"    å“åº”è§£æå¼‚å¸¸: {e}")
            return default_scores
    
    def evaluate_single_question(self, context: str, model: str, question_id: str) -> Dict[str, int]:
        """
        ä½¿ç”¨å•ä¸ªæ¨¡å‹è¯„ä¼°å•é“é¢˜
        
        Args:
            context: è¯„ä¼°ä¸Šä¸‹æ–‡
            model: ä½¿ç”¨çš„æ¨¡å‹
            question_id: é¢˜ç›®ID
            
        Returns:
            è¯¥é¢˜åœ¨å„ç»´åº¦ä¸Šçš„è¯„åˆ†
        """
        print(f"    â””â”€ ä½¿ç”¨æ¨¡å‹ {model} è¯„ä¼° {question_id}...")
        
        try:
            response = ollama.generate(model=model, prompt=context, options={'num_predict': 1000})
            scores = self.parse_scores_from_response(response['response'])
            return scores
        except Exception as e:
            print(f"    âŒ æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥: {e}")
            return {
                'openness_to_experience': 3,
                'conscientiousness': 3,
                'extraversion': 3,
                'agreeableness': 3,
                'neuroticism': 3
            }
    
    def detect_disputes(self, scores_list: List[Dict[str, int]], threshold: float = 1.0) -> Dict[str, List]:
        """æ£€æµ‹è¯„åˆ†äº‰è®®ï¼ˆæ‰€æœ‰ç»´åº¦ï¼‰"""
        disputes = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        
        for trait in traits:
            trait_scores = [scores[trait] for scores in scores_list if trait in scores]
            if len(trait_scores) > 1:
                score_range = max(trait_scores) - min(trait_scores)
                if score_range > threshold:
                    disputes[trait] = {
                        'scores': trait_scores,
                        'range': score_range,
                        'requires_resolution': True
                    }
        
        return disputes
    
    def detect_major_dimension_disputes(self, scores_list: List[Dict[str, int]], question: Dict, threshold: float = 1.0) -> Dict[str, List]:
        """
        æ£€æµ‹ä¸»è¦ç»´åº¦è¯„åˆ†äº‰è®®ï¼ˆåªæ£€æŸ¥é¢˜ç›®æ‰€å±çš„ä¸»è¦ç»´åº¦ï¼‰
        """
        question_data = question.get('question_data', {})
        primary_dimension = question_data.get('dimension', '')
        
        # å°†ä¸»è¦ç»´åº¦æ˜ å°„åˆ°æ ‡å‡†åç§°
        dimension_map = {
            'Extraversion': 'extraversion',
            'Agreeableness': 'agreeableness', 
            'Conscientiousness': 'conscientiousness',
            'Neuroticism': 'neuroticism',
            'Openness to Experience': 'openness_to_experience'
        }
        
        standard_primary_dimension = dimension_map.get(primary_dimension, '')
        
        if not standard_primary_dimension:
            # å¦‚æœæ— æ³•ç¡®å®šä¸»è¦ç»´åº¦ï¼Œè¿”å›æ‰€æœ‰äº‰è®®
            return self.detect_disputes(scores_list, threshold)
        
        # åªæ£€æŸ¥ä¸»è¦ç»´åº¦çš„äº‰è®®
        disputes = {}
        trait_scores = [scores[standard_primary_dimension] for scores in scores_list if standard_primary_dimension in scores]
        if len(trait_scores) > 1:
            score_range = max(trait_scores) - min(trait_scores)
            if score_range > threshold:
                disputes[standard_primary_dimension] = {
                    'scores': trait_scores,
                    'range': score_range,
                    'requires_resolution': True
                }
        
        return disputes
    
    def process_single_question(self, question: Dict, question_idx: int) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªé¢˜ç›®ï¼Œæä¾›è¯¦ç»†åé¦ˆ
        
        Args:
            question: é¢˜ç›®ä¿¡æ¯
            question_idx: é¢˜ç›®ç´¢å¼•
            
        Returns:
            å¤„ç†ç»“æœ
        """
        question_id = question.get('question_id', 'Unknown')
        question_concept = question['question_data'].get('mapped_ipip_concept', 'Unknown')
        
        # ç¡®ä¿question_idæ˜¯å­—ç¬¦ä¸²
        if not isinstance(question_id, str):
            question_id = str(question_id)
        
        is_reversed = self.reverse_processor.is_reverse_item(question_id) or \
                     self.reverse_processor.is_reverse_from_concept(question_concept)
        
        print(f"å¤„ç†ç¬¬ {question_idx+1:02d} é¢˜ (ID: {question_id})")
        print(f"  é¢˜ç›®æ¦‚å¿µ: {question_concept}")
        print(f"  æ˜¯å¦åå‘: {is_reversed}")
        print(f"  è¢«è¯•å›ç­”: {question['extracted_response'][:100]}...")
        
        # ç”Ÿæˆè¯„ä¼°ä¸Šä¸‹æ–‡
        context = self.context_generator.generate_evaluation_prompt(question)
        
        # åˆå§‹è¯„ä¼°ï¼ˆä½¿ç”¨3ä¸ªä¸»è¦æ¨¡å‹ï¼‰
        print(f"  åˆå§‹è¯„ä¼° (ä½¿ç”¨ {len(self.primary_models)} ä¸ªæ¨¡å‹):")
        initial_scores = []
        for model in self.primary_models:
            scores = self.evaluate_single_question(context, model, question_id)
            initial_scores.append({
                'model': model,
                'scores': scores,
                'raw_scores': scores.copy()  # ä¿å­˜åŸå§‹è¯„åˆ†
            })
            time.sleep(0.5)  # é˜²æ­¢APIè¿‡è½½
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨äº‰è®®ï¼ˆåªæ£€æŸ¥ä¸»è¦ç»´åº¦ï¼‰
        all_initial_scores = [item['scores'] for item in initial_scores]
        disputes = self.detect_major_dimension_disputes(all_initial_scores, question, self.dispute_threshold)
        
        print(f"  äº‰è®®æ£€æµ‹: {len(disputes)} ä¸ªä¸»è¦ç»´åº¦å­˜åœ¨åˆ†æ­§")
        if disputes:
            for trait, dispute_info in disputes.items():
                print(f"    - {trait}: è¯„åˆ† {dispute_info['scores']}, å·®è· {dispute_info['range']}")
        else:
            print(f"    æ— é‡å¤§åˆ†æ­§")
        
        # äº‰è®®è§£å†³ï¼ˆæ¯è½®è¿½åŠ 2ä¸ªæ¨¡å‹ï¼Œæœ€å¤š3è½®ï¼‰
        current_scores = initial_scores.copy()
        resolution_round = 0
        all_models_used = [item['model'] for item in initial_scores]
        all_scores_data = all_initial_scores.copy()
        
        while disputes and resolution_round < self.max_dispute_rounds:
            print(f"  ç¬¬ {resolution_round + 1} è½®äº‰è®®è§£å†³:")
            
            # æ¯è½®è¿½åŠ 2ä¸ªäº‰è®®è§£å†³æ¨¡å‹
            dispute_models_for_round = []
            for i in range(2):  # æ¯è½®2ä¸ªæ¨¡å‹
                model_index = (resolution_round * 2 + i) % len(self.dispute_models)
                dispute_models_for_round.append(self.dispute_models[model_index])
            
            print(f"    ä½¿ç”¨è¿½åŠ æ¨¡å‹: {dispute_models_for_round}")
            
            # ä¸ºæ¯è½®çš„2ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
            for dispute_model in dispute_models_for_round:
                print(f"    ä½¿ç”¨æ¨¡å‹ {dispute_model}:")
                new_scores = self.evaluate_single_question(context, dispute_model, question_id)
                
                # æ·»åŠ åˆ°è¯„åˆ†è®°å½•
                current_scores.append({
                    'model': dispute_model,
                    'scores': new_scores,
                    'raw_scores': new_scores.copy()
                })
                all_models_used.append(dispute_model)
                all_scores_data.append(new_scores)
            
            # é‡æ–°æ£€æµ‹äº‰è®®ï¼ˆåªæ£€æŸ¥ä¸»è¦ç»´åº¦ï¼‰
            major_disputes = self.detect_major_dimension_disputes(all_scores_data, question, self.dispute_threshold)
            disputes = major_disputes
            resolution_round += 1
            
            if disputes:
                print(f"    ä»å­˜åœ¨ {len(disputes)} ä¸ªä¸»è¦ç»´åº¦åˆ†æ­§: {list(disputes.keys())}")
                # æ˜¾ç¤ºæ¯ä¸ªäº‰è®®çš„è¯¦ç»†ä¿¡æ¯
                for trait, dispute_info in disputes.items():
                    print(f"      {trait}: {dispute_info['scores']}")
            else:
                print(f"    æ‰€æœ‰ä¸»è¦ç»´åº¦åˆ†æ­§å·²è§£å†³")
        
        # åº”ç”¨å¤šæ•°å†³ç­–åŸåˆ™ç¡®å®šæœ€ç»ˆåŸå§‹è¯„åˆ†
        final_raw_scores = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        
        for trait in traits:
            trait_scores = [scores_data[trait] for scores_data in all_scores_data if trait in scores_data]
            if trait_scores:
                # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆè¯„åˆ†
                median_score = statistics.median(trait_scores)
                final_raw_scores[trait] = int(round(median_score))  # ç¡®ä¿æ˜¯æ•´æ•°
            else:
                final_raw_scores[trait] = 3  # é»˜è®¤å€¼
        
        print(f"  åŸå§‹æœ€ç»ˆè¯„åˆ†: {final_raw_scores}")
        
        # åº”ç”¨åå‘è®¡åˆ†è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if is_reversed:
            final_adjusted_scores = {}
            print(f"  åº”ç”¨åå‘è®¡åˆ†è½¬æ¢:")
            for trait, raw_score in final_raw_scores.items():
                adjusted_score = self.reverse_processor.reverse_score(raw_score)
                final_adjusted_scores[trait] = adjusted_score
                if raw_score != adjusted_score:
                    print(f"    {trait}: {raw_score} â†’ {adjusted_score}")
                else:
                    print(f"    {trait}: {raw_score} (ä¸å˜)")
        else:
            final_adjusted_scores = final_raw_scores
            print(f"  éåå‘é¢˜ç›®ï¼Œæ— éœ€è½¬æ¢: {final_adjusted_scores}")
        
        print(f"  æœ€ç»ˆè¯„åˆ†: {final_adjusted_scores}")
        print(f"  ä½¿ç”¨æ¨¡å‹: {all_models_used}")
        print(f"  äº‰è®®è§£å†³è½®æ¬¡: {resolution_round}")
        print(f"  è¯„åˆ†æ€»æ•°: {len(all_scores_data)}")
        print()
        
        return {
            'question_id': question_id,
            'question_info': question,
            'initial_scores': initial_scores,
            'final_raw_scores': final_raw_scores,
            'final_adjusted_scores': final_adjusted_scores,
            'resolution_rounds': resolution_round,
            'disputes_initial': len(self.detect_disputes([item['scores'] for item in initial_scores])),
            'disputes_final': len(disputes),
            'models_used': all_models_used,
            'is_reversed': is_reversed,
            'scores_data': all_scores_data
        }
    
    def calculate_big5_scores(self, question_results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å¤§äº”äººæ ¼å„ç»´åº¦å¾—åˆ†ï¼ˆå¸¦æƒé‡ï¼‰"""
        print("å¼€å§‹è®¡ç®—å¤§äº”äººæ ¼å¾—åˆ†ï¼ˆå¸¦æƒé‡ï¼‰:")
        
        # æŒ‰ç»´åº¦æ”¶é›†åˆ†æ•°å’Œæƒé‡
        scores_by_dimension = {
            'openness_to_experience': [],
            'conscientiousness': [],
            'extraversion': [],
            'agreeableness': [],
            'neuroticism': []
        }
        
        # æ”¶é›†æ¯é“é¢˜çš„ä¸»è¦ç»´åº¦ä¿¡æ¯
        for result in question_results:
            scores = result['final_adjusted_scores']  # ä½¿ç”¨è°ƒæ•´ååˆ†æ•°
            question_info = result.get('question_info', {})
            question_data = question_info.get('question_data', {})
            primary_dimension = question_data.get('dimension', '')  # é¢˜ç›®ä¸»è¦ç»´åº¦
            
            # å°†ä¸»è¦ç»´åº¦æ˜ å°„åˆ°æ ‡å‡†åç§°
            dimension_map = {
                'Openness to Experience': 'openness_to_experience',
                'Conscientiousness': 'conscientiousness',
                'Extraversion': 'extraversion',
                'Agreeableness': 'agreeableness',
                'Neuroticism': 'neuroticism'
            }
            
            standard_primary_dimension = dimension_map.get(primary_dimension, '')
            
            # ä¸ºæ¯ä¸ªç»´åº¦æ·»åŠ å¸¦æƒé‡çš„åˆ†æ•°
            for dimension in scores_by_dimension:
                if dimension in scores:
                    score = scores[dimension]
                    if score in [1, 3, 5]:  # ç¡®ä¿æ˜¯æœ‰æ•ˆåˆ†æ•°
                        # è®¡ç®—æƒé‡ï¼šä¸»è¦ç»´åº¦70%ï¼Œå…¶ä»–ç»´åº¦å„7.5%
                        if dimension == standard_primary_dimension and standard_primary_dimension:
                            weight = 0.7  # ä¸»è¦ç»´åº¦é«˜æƒé‡
                        else:
                            weight = 0.075  # å…¶ä»–ç»´åº¦ä½æƒé‡
                        
                        scores_by_dimension[dimension].append({
                            'score': score,
                            'weight': weight,
                            'is_primary': (dimension == standard_primary_dimension and standard_primary_dimension)
                        })
        
        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        big5_scores = {}
        for dimension, weighted_scores in scores_by_dimension.items():
            if weighted_scores:
                # è®¡ç®—åŠ æƒå¹³å‡
                total_weighted_score = sum(item['score'] * item['weight'] for item in weighted_scores)
                total_weight = sum(item['weight'] for item in weighted_scores)
                
                if total_weight > 0:
                    weighted_avg = total_weighted_score / total_weight
                    # éä¸»è¦ç»´åº¦åˆ†æ•°è®¡ç®—å¹³å‡åä¹˜ä»¥æƒé‡
                    primary_scores = [item['score'] for item in weighted_scores if item['is_primary']]
                    other_scores = [item['score'] for item in weighted_scores if not item['is_primary']]
                    
                    # å¦‚æœæœ‰ä¸»è¦ç»´åº¦åˆ†æ•°
                    if primary_scores:
                        primary_avg = sum(primary_scores) / len(primary_scores)
                        # å¦‚æœæœ‰å…¶ä»–ç»´åº¦åˆ†æ•°ï¼Œä¹Ÿè¦è®¡ç®—å¹³å‡
                        if other_scores:
                            other_avg = sum(other_scores) / len(other_scores)
                            # åŠ æƒè®¡ç®—
                            weighted_score = 0.7 * primary_avg + 0.3 * other_avg
                        else:
                            weighted_score = primary_avg
                    else:
                        # å¦‚æœæ²¡æœ‰ä¸»è¦ç»´åº¦åˆ†æ•°ï¼Œè®¡ç®—æ‰€æœ‰åˆ†æ•°å¹³å‡
                        all_scores = [item['score'] for item in weighted_scores]
                        weighted_score = sum(all_scores) / len(all_scores)
                    
                    big5_scores[dimension] = round(weighted_score, 2)
                    print(f"  {dimension}:")
                    if primary_scores:
                        print(f"    ä¸»è¦ç»´åº¦å¹³å‡: {sum(primary_scores) / len(primary_scores):.2f} (n={len(primary_scores)})")
                    if other_scores:
                        print(f"    å…¶ä»–ç»´åº¦å¹³å‡: {sum(other_scores) / len(other_scores):.2f} (n={len(other_scores)})")
                    print(f"    åŠ æƒæ€»åˆ†: {weighted_score:.2f}")
                else:
                    big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
                    print(f"  {dimension}: æ— è¯„åˆ†æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼3.0")
            else:
                big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
                print(f"  {dimension}: æ— è¯„åˆ†æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼3.0")
        
        return big5_scores
    
    def calculate_mbti_type(self, big5_scores: Dict[str, float]) -> str:
        """åŸºäºå¤§äº”äººæ ¼å¾—åˆ†æ¨æ–­MBTIç±»å‹"""
        print("æ¨æ–­MBTIç±»å‹:")
        
        # ç®€åŒ–çš„MBTIæ¨æ–­é€»è¾‘
        O = big5_scores.get('openness_to_experience', 3)
        C = big5_scores.get('conscientiousness', 3)
        E = big5_scores.get('extraversion', 3)
        A = big5_scores.get('agreeableness', 3)
        N = big5_scores.get('neuroticism', 3)
        
        # E/Iç»´åº¦ï¼šå¤–å‘æ€§ vs å†…å‘æ€§ï¼ˆåŒ…å«ç¥ç»è´¨å› ç´ ï¼‰
        e_score = E + (5 - N)  # é«˜å¤–å‘æ€§+ä½ç¥ç»è´¨=å¤–å‘
        i_score = (5 - E) + N  # é«˜ç¥ç»è´¨+ä½å¤–å‘æ€§=å†…å‘
        E_preference = 'E' if e_score > i_score else 'I'
        
        # S/Nç»´åº¦ï¼šå¼€æ”¾æ€§ï¼ˆé€šå¸¸å¼€æ”¾æ€§é«˜=ç›´è§‰Nï¼Œå¼€æ”¾æ€§ä½=æ„Ÿè§‰Sï¼‰
        S_preference = 'S' if O <= 3 else 'N'
        
        # T/Fç»´åº¦ï¼šå®œäººæ€§ï¼ˆå®œäººæ€§é«˜=Fï¼Œå®œäººæ€§ä½=Tï¼‰
        T_preference = 'T' if A <= 3 else 'F'
        
        # J/Pç»´åº¦ï¼šå°½è´£æ€§ï¼ˆå°½è´£æ€§é«˜=åˆ¤æ–­Jï¼Œå°½è´£æ€§ä½=çŸ¥è§‰Pï¼‰
        J_preference = 'J' if C > 3 else 'P'
        
        mbti_type = f"{E_preference}{S_preference}{T_preference}{J_preference}"
        
        print(f"  E/I: E({E}) vs I({5-E}) + N({N}) â†’ {E_preference}")
        print(f"  S/N: O({O}) â†’ {S_preference}")
        print(f"  T/F: A({A}) â†’ {T_preference}") 
        print(f"  J/P: C({C}) â†’ {J_preference}")
        print(f"  MBTIç±»å‹: {mbti_type}")
        
        return mbti_type
    
    def process_single_report(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Š
        
        Args:
            file_path: æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"å¤„ç†æµ‹è¯„æŠ¥å‘Š: {file_path}")
        print("-" * 80)
        
        start_time = time.time()
        
        try:
            # è§£æè¾“å…¥æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            assessment_results = data.get('assessment_results', [])
            
            if not assessment_results:
                print(f"âŒ æœªæ‰¾åˆ°assessment_resultså­—æ®µ")
                return None
            
            print(f"æ‰¾åˆ° {len(assessment_results)} é“é¢˜ç›®")
            
            # å¤„ç†æ¯é“é¢˜
            question_results = []
            for i, question in enumerate(assessment_results):
                result = self.process_single_question(question, i)
                question_results.append(result)
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š è¿›åº¦: {i + 1}/{len(assessment_results)} é¢˜å·²å¤„ç†")
            
            # è®¡ç®—å¤§äº”äººæ ¼å¾—åˆ†
            print(f"å¼€å§‹è®¡ç®—å¤§äº”äººæ ¼å¾—åˆ†...")
            big5_scores = self.calculate_big5_scores(question_results)
            
            # æ¨æ–­MBTIç±»å‹
            mbti_type = self.calculate_mbti_type(big5_scores)
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡æ•°æ®
            total_time = time.time() - start_time
            reversed_count = sum(1 for r in question_results if r['is_reversed'])
            disputed_count = sum(1 for r in question_results if r['resolution_rounds'] > 0)
            models_called = sum(len(r['models_used']) for r in question_results)
            
            result = {
                'success': True,
                'file_path': file_path,
                'processing_time': round(total_time, 1),
                'big5_scores': big5_scores,
                'mbti_type': mbti_type,
                'question_results': question_results,
                'summary': {
                    'total_questions': len(assessment_results),
                    'reversed_count': reversed_count,
                    'disputed_count': disputed_count,
                    'models_called': models_called,
                    'average_time_per_question': round(total_time / len(assessment_results), 1) if assessment_results else 0
                }
            }
            
            print(f"\nå¤„ç†å®Œæˆ!")
            print(f"æ€»æ—¶é—´: {total_time:.1f} ç§’")
            print(f"å¤§äº”äººæ ¼å¾—åˆ†: {big5_scores}")
            print(f"MBTIç±»å‹: {mbti_type}")
            print(f"åå‘é¢˜ç›®æ•°: {reversed_count}")
            print(f"äº‰è®®è§£å†³æ•°: {disputed_count}")
            
            return result
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("ç‹¬ç«‹æ‰¹é‡å¤„ç†å™¨ - æ¼”ç¤ºæ¨¡å¼")
    print("="*60)
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = StandaloneBatchProcessor()
    
    # ç¤ºä¾‹æ–‡ä»¶è·¯å¾„
    sample_file = r"..\results\readonly-original\asses_gemma3_latest_agent_big_five_50_complete2_def_e0_t0_0_09201.json"
    
    if os.path.exists(sample_file):
        print(f"æ‰¾åˆ°ç¤ºä¾‹æ–‡ä»¶: {sample_file}")
        
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        result = processor.process_single_report(sample_file)
        
        if result and result.get('success', False):
            print(f"\nâœ… æ–‡ä»¶å¤„ç†æˆåŠŸ!")
            print(f"å¤§äº”äººæ ¼å¾—åˆ†: {result['big5_scores']}")
            print(f"MBTIç±»å‹: {result['mbti_type']}")
            print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.1f}ç§’")
        else:
            print(f"\nâŒ æ–‡ä»¶å¤„ç†å¤±è´¥!")
            error_msg = result.get('error', 'Unknown error') if result else 'No result'
            print(f"é”™è¯¯: {error_msg}")
    else:
        print(f"âŒ ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {sample_file}")
        print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ç»“æ„ä¸‹è¿è¡Œæ­¤è„šæœ¬")


if __name__ == "__main__":
    main()