#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºé€æ˜æµæ°´çº¿ - é›†æˆæ–°çš„é€‚åº”æ€§å…±è¯†ç®—æ³•å’Œå¯é æ€§è®¡ç®—å™¨
åŸºäºTDDçš„æœ€å°åŒ–æ›¿æ¢å®ç°
"""

import json
import ollama
from typing import Dict, List, Any
from .context_generator import ContextGenerator
from .reverse_scoring_processor import ReverseScoringProcessor
from .input_parser import InputParser
import time
import statistics
import re

# å¯¼å…¥æ–°çš„ç®—æ³•ç»„ä»¶
from adaptive_consensus_algorithm import AdaptiveConsensusAlgorithm
from adaptive_reliability_calculator import AdaptiveReliabilityCalculator


class EnhancedTransparentPipeline:
    """
    å¢å¼ºé€æ˜æµæ°´çº¿ - é›†æˆæ–°å…±è¯†ç®—æ³•å’Œå¯é æ€§è®¡ç®—å™¨

    ä¸»è¦æ”¹è¿›ï¼š
    1. ä½¿ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•æ›¿ä»£åŸæœ‰çš„äº‰è®®è§£å†³æœºåˆ¶
    2. ä½¿ç”¨å››ç»´å¯é æ€§è®¡ç®—å™¨æä¾›æ›´ç§‘å­¦çš„å¯é æ€§è¯„ä¼°
    3. ä¿æŒä¸åŸæµæ°´çº¿çš„å…¼å®¹æ€§ï¼Œæœ€å°åŒ–æ›¿æ¢
    """

    def __init__(self, primary_models: List[str] = None, dispute_models: List[str] = None, use_cloud: bool = True):
        """
        åˆå§‹åŒ–å¢å¼ºæµæ°´çº¿

        Args:
            primary_models: ä¸»è¦è¯„ä¼°æ¨¡å‹åˆ—è¡¨
            dispute_models: äº‰è®®è§£å†³æ¨¡å‹åˆ—è¡¨
            use_cloud: æ˜¯å¦ä½¿ç”¨äº‘ç«¯æ¨¡å‹
        """
        self.use_cloud = use_cloud

        if use_cloud:
            # äº‘ç«¯ä¼˜å…ˆé…ç½®
            self.primary_models = primary_models or [
                'deepseek-v3.1:671b-cloud',  # 671Bå‚æ•°ï¼Œä¸»åŠ›æ¨¡å‹
                'gpt-oss:120b-cloud',       # 120Bå‚æ•°ï¼Œç‹¬ç«‹éªŒè¯
                'qwen3-vl:235b-cloud'       # 235Bå‚æ•°ï¼Œé«˜è´¨é‡è¡¥å……
            ]

            self.dispute_models = dispute_models or [
                'qwen3-vl:235b-cloud',       # é«˜è´¨é‡äº‰è®®è§£å†³
                'gpt-oss:120b-cloud',       # æœ€ç»ˆä»²è£
                'qwen3:8b',                  # æœ¬åœ°å¤‡ä»½1
                'deepseek-r1:8b'            # æœ¬åœ°å¤‡ä»½2
            ]
        else:
            # æœ¬åœ°æ¨¡å‹é…ç½®
            self.primary_models = primary_models or [
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral-nemo:latest'
            ]

            self.dispute_models = dispute_models or [
                'llama3:latest',      # Meta (ç¬¬1è½®ç¬¬1ä¸ª)
                'gemma3:latest',      # Google (ç¬¬1è½®ç¬¬2ä¸ª)
                'phi3:mini',          # Microsoft (ç¬¬2è½®ç¬¬1ä¸ª)
                'yi:6b',              # 01.AI (ç¬¬2è½®ç¬¬2ä¸ª)
                'qwen3:4b',           # Alibaba (ç¬¬3è½®ç¬¬1ä¸ª)
                'deepseek-r1:8b',     # DeepSeek (ç¬¬3è½®ç¬¬2ä¸ª)
                'mixtral:8x7b'        # Mistral (å¤‡ç”¨)
            ]

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.context_generator = ContextGenerator()
        self.reverse_processor = ReverseScoringProcessor()
        self.input_parser = InputParser()

        # åˆå§‹åŒ–æ–°çš„ç®—æ³•ç»„ä»¶
        self.consensus_algorithm = AdaptiveConsensusAlgorithm()
        self.reliability_calculator = AdaptiveReliabilityCalculator()

        self.max_dispute_rounds = 3
        self.dispute_threshold = 1.0

    def parse_scores_from_response(self, response: str) -> Dict[str, int]:
        """ä»æ¨¡å‹å“åº”ä¸­è§£æè¯„åˆ†"""
        import json

        # å°è¯•æŸ¥æ‰¾JSONéƒ¨åˆ†
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0)
                data = json.loads(json_str)

                if 'scores' in data:
                    scores = data['scores']
                    # ç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½æ˜¯1ã€3ã€5ä¸­çš„ä¸€ä¸ª
                    for trait, score in scores.items():
                        if isinstance(score, (int, float)):
                            if score <= 2:
                                scores[trait] = 1
                            elif score <= 4:
                                scores[trait] = 3
                            else:
                                scores[trait] = 5
                        else:
                            scores[trait] = 3  # é»˜è®¤å€¼
                    return scores
            except json.JSONDecodeError:
                pass

        # å¦‚æœæ‰¾ä¸åˆ°JSONï¼Œè¿”å›é»˜è®¤å€¼
        return {
            'openness_to_experience': 3,
            'conscientiousness': 3,
            'extraversion': 3,
            'agreeableness': 3,
            'neuroticism': 3
        }

    def evaluate_single_question_with_fallback(self, context: str, model: str, question_id: str) -> Dict[str, int]:
        """
        ä½¿ç”¨å•ä¸ªæ¨¡å‹è¯„ä¼°å•é“é¢˜ï¼Œæä¾›æ™ºèƒ½å›é€€ï¼Œç»å¯¹ç¦æ­¢é»˜è®¤è¯„åˆ†
        """
        print(f"    â””â”€ ä½¿ç”¨æ¨¡å‹ {model} è¯„ä¼°é¢˜ç›® {question_id}...")

        # å®šä¹‰å›é€€æ¨¡å‹åˆ—è¡¨ï¼ˆäº‘ç«¯ä¼˜å…ˆï¼Œæœ¬åœ°å¤‡ä»½ï¼‰
        if model.endswith('-cloud'):
            fallback_models = [
                model,  # é¦–é€‰äº‘ç«¯æ¨¡å‹
                # å…¶ä»–äº‘ç«¯æ¨¡å‹
                'gpt-oss:120b-cloud',
                'qwen3-vl:235b-cloud',
                # æœ¬åœ°æ¨¡å‹
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral:instruct'
            ]
        else:
            fallback_models = [
                model,  # é¦–é€‰æœ¬åœ°æ¨¡å‹
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral:instruct'
            ]

        last_error = None

        for attempt_model in fallback_models:
            try:
                print(f"      å°è¯•ä½¿ç”¨æ¨¡å‹: {attempt_model}")

                # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
                if attempt_model.endswith('-cloud'):
                    time.sleep(2)
                else:
                    time.sleep(0.5)

                response = ollama.generate(model=attempt_model, prompt=context, options={'num_predict': 2000})
                scores = self.parse_scores_from_response(response['response'])

                # éªŒè¯è¯„åˆ†æœ‰æ•ˆæ€§
                if self._validate_scores(scores):
                    print(f"      âœ… è¯„åˆ†æˆåŠŸ: {scores}")
                    return scores
                else:
                    print(f"      âš ï¸ è¯„åˆ†æ— æ•ˆ: {scores}")
                    continue

            except Exception as e:
                last_error = str(e)
                error_msg = str(e).lower()

                # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•
                if "usage limit" in error_msg or "402" in error_msg:
                    print(f"      âŒ æ¨¡å‹ {attempt_model} APIé™åˆ¶: {e}")
                    # APIé™åˆ¶é”™è¯¯ï¼Œè·³è¿‡å»¶è¿Ÿç›´æ¥å°è¯•ä¸‹ä¸€ä¸ª
                    continue
                elif "502" in error_msg or "500" in error_msg or "eof" in error_msg:
                    print(f"      âŒ æ¨¡å‹ {attempt_model} æœåŠ¡é”™è¯¯: {e}")
                    time.sleep(3)  # æœåŠ¡é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    continue
                else:
                    print(f"      âŒ æ¨¡å‹ {attempt_model} å…¶ä»–é”™è¯¯: {e}")
                    time.sleep(1)
                    continue

        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›é»˜è®¤å€¼
        raise RuntimeError(f"æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•è¯„ä¼°é¢˜ç›® {question_id}ï¼Œæœ€åé”™è¯¯: {last_error}")

    def _validate_scores(self, scores: Dict[str, int]) -> bool:
        """éªŒè¯è¯„åˆ†çš„æœ‰æ•ˆæ€§"""
        if not isinstance(scores, dict):
            return False

        required_traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for trait in required_traits:
            if trait not in scores:
                return False
            if not isinstance(scores[trait], (int, float)):
                return False
            if not (1 <= scores[trait] <= 5):
                return False

        return True

    def evaluate_single_question(self, context: str, model: str, question_id: str) -> Dict[str, int]:
        """
        ä½¿ç”¨å•ä¸ªæ¨¡å‹è¯„ä¼°å•é“é¢˜ï¼Œå¹¶æä¾›è¯¦ç»†åé¦ˆ
        """
        return self.evaluate_single_question_with_fallback(context, model, question_id)

    def process_single_question_with_new_algorithms(self, question: Dict, question_idx: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ–°å…±è¯†ç®—æ³•å’Œå¯é æ€§è®¡ç®—å™¨å¤„ç†å•é“é¢˜

        è¿™æ˜¯æ ¸å¿ƒæ›¿æ¢ï¼šåŸæœ‰çš„äº‰è®®è§£å†³æœºåˆ¶è¢«é€‚åº”æ€§å…±è¯†ç®—æ³•æ›¿ä»£
        """
        question_id = question.get('question_id', 'Unknown')
        question_concept = question['question_data'].get('mapped_ipip_concept', 'Unknown')

        # ç¡®ä¿question_idæ˜¯å­—ç¬¦ä¸²
        if not isinstance(question_id, str):
            question_id = str(question_id)

        is_reversed = self.reverse_processor.is_reverse_item(question_id) or \
                     self.reverse_processor.is_reverse_from_concept(question_concept)

        print(f"å¤„ç†ç¬¬ {question_idx+1:02d} é¢˜ (ID: {question_id}) - ä½¿ç”¨æ–°ç®—æ³•")
        print(f"  é¢˜ç›®æ¦‚å¿µ: {question_concept}")
        print(f"  æ˜¯å¦åå‘: {is_reversed}")
        print(f"  è¢«è¯•å›ç­”: {question['extracted_response'][:100]}...")

        # ç”Ÿæˆè¯„ä¼°ä¸Šä¸‹æ–‡
        context = self.context_generator.generate_evaluation_prompt(question)

        # æ–°ç®—æ³•1ï¼šé€‚åº”æ€§å…±è¯†ç®—æ³•å¤„ç†
        print(f"  ä½¿ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•è¿›è¡Œè¯„ä¼°:")

        # åˆ›å»ºåŠ¨æ€è¯„ä¼°å™¨å‡½æ•°
        def adaptive_evaluator(required_count: int) -> List[int]:
            """
            åŠ¨æ€è¯„ä¼°å™¨ï¼šæ ¹æ®å…±è¯†ç®—æ³•éœ€æ±‚è·å–æ–°è¯„åˆ†

            è¿™æ˜¯ä¸åŸæµæ°´çº¿çš„å…³é”®å·®å¼‚ï¼š
            åŸæµæ°´çº¿ï¼šå›ºå®šè½®æ¬¡ï¼Œæ¯è½®2ä¸ªæ¨¡å‹
            æ–°ç®—æ³•ï¼šæŒ‰éœ€åŠ¨æ€è·å–è¯„ä¼°å™¨è¯„åˆ†
            """
            print(f"    ğŸ“ å…±è¯†ç®—æ³•è¯·æ±‚ {required_count} ä¸ªæ–°è¯„åˆ†")

            new_scores = []
            models_used = []

            # æ ¹æ®éœ€è¦çš„æ•°é‡é€‰æ‹©æ¨¡å‹
            for i in range(required_count):
                # å¾ªç¯ä½¿ç”¨äº‰è®®è§£å†³æ¨¡å‹
                model_index = i % len(self.dispute_models)
                model = self.dispute_models[model_index]

                try:
                    scores = self.evaluate_single_question(context, model, f"{question_id}_adaptive_{i}")

                    # æ–°å…±è¯†ç®—æ³•åªéœ€è¦å•ä¸€è¯„åˆ†ï¼Œä½¿ç”¨é¢˜ç›®ä¸»è¦ç»´åº¦
                    primary_dimension = self._get_primary_dimension(question)
                    single_score = scores.get(primary_dimension, 3)

                    # ç¡®ä¿è¯„åˆ†æ˜¯1,3,5
                    if single_score not in [1, 3, 5]:
                        if single_score <= 2:
                            single_score = 1
                        elif single_score >= 4:
                            single_score = 5
                        else:
                            single_score = 3

                    new_scores.append(single_score)
                    models_used.append(model)
                    print(f"      âœ… {model}: {single_score}")

                except Exception as e:
                    print(f"      âŒ {model}: {e}")
                    # ä½¿ç”¨é»˜è®¤å€¼ä½†ä¸å½±å“ç®—æ³•æµç¨‹
                    new_scores.append(3)
                    models_used.append(model)

            print(f"    ğŸ“Š æ–°è¯„åˆ†è·å–å®Œæˆ: {new_scores} (æ¨¡å‹: {models_used})")
            return new_scores

        # è·å–åˆå§‹3ä¸ªè¯„ä¼°å™¨è¯„åˆ†
        print(f"  è·å–åˆå§‹3ä¸ªè¯„ä¼°å™¨è¯„åˆ†:")
        initial_scores = []
        initial_models_used = []

        for i in range(3):
            model = self.primary_models[i % len(self.primary_models)]
            try:
                scores = self.evaluate_single_question(context, model, f"{question_id}_init_{i}")

                # ä½¿ç”¨é¢˜ç›®ä¸»è¦ç»´åº¦ä½œä¸ºå•ä¸€è¯„åˆ†
                primary_dimension = self._get_primary_dimension(question)
                single_score = scores.get(primary_dimension, 3)

                # ç¡®ä¿è¯„åˆ†æ˜¯1,3,5
                if single_score not in [1, 3, 5]:
                    if single_score <= 2:
                        single_score = 1
                    elif single_score >= 4:
                        single_score = 5
                    else:
                        single_score = 3

                initial_scores.append(single_score)
                initial_models_used.append(model)
                print(f"    âœ… {model}: {single_score}")

            except Exception as e:
                print(f"    âŒ {model}: {e}")
                initial_scores.append(3)
                initial_models_used.append(model)

        print(f"  ğŸ“Š åˆå§‹è¯„åˆ†: {initial_scores}")

        # åº”ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•
        print(f"  ğŸ§  åº”ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•:")
        consensus_result = self.consensus_algorithm.adaptive_consensus(initial_scores, adaptive_evaluator)

        print(f"  âœ… å…±è¯†ç®—æ³•å®Œæˆ:")
        print(f"    å…±è¯†è¯„åˆ†: {consensus_result['consensus_score']}")
        print(f"    å…±è¯†æ–¹æ³•: {consensus_result['consensus_method']}")
        print(f"    å¤„ç†è½®æ•°: {consensus_result['processing_rounds']}")
        print(f"    æœ€ç»ˆè¯„åˆ†: {consensus_result['final_scores']}")

        # æ–°ç®—æ³•2ï¼šé€‚åº”æ€§å¯é æ€§è®¡ç®—
        print(f"  ğŸ”§ è®¡ç®—é€‚åº”æ€§å¯é æ€§:")
        reliability_result = self.reliability_calculator.calculate_adaptive_reliability(
            consensus_result, initial_scores
        )

        print(f"  âœ… å¯é æ€§è®¡ç®—å®Œæˆ:")
        print(f"    æ€»ä½“å¯é æ€§: {reliability_result['overall_reliability']:.3f}")
        print(f"    å…±è¯†è´¨é‡: {reliability_result['consensus_quality']:.3f}")
        print(f"    è¯„ä¼°å™¨å¤šæ ·æ€§: {reliability_result['evaluator_diversity']:.3f}")
        print(f"    å¤„ç†æ•ˆç‡: {reliability_result['processing_efficiency']:.3f}")
        print(f"    æœ€ç»ˆä¸€è‡´æ€§: {reliability_result['final_agreement']:.3f}")

        # å°†å•ä¸€å…±è¯†è¯„åˆ†æ‰©å±•åˆ°æ‰€æœ‰ç»´åº¦ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        final_adjusted_scores = self._expand_consensus_score_to_all_dimensions(
            consensus_result['consensus_score'], question
        )

        # åº”ç”¨åå‘è®¡åˆ†è½¬æ¢
        if is_reversed:
            final_adjusted_scores = {
                trait: self.reverse_processor.reverse_score(score)
                for trait, score in final_adjusted_scores.items()
            }
            print(f"  åº”ç”¨åå‘è®¡åˆ†è½¬æ¢: {final_adjusted_scores}")

        print(f"  æœ€ç»ˆè¯„åˆ†: {final_adjusted_scores}")
        print(f"  æ€»ä½“å¯é æ€§: {reliability_result['overall_reliability']:.3f}")
        print()

        return {
            'question_id': question_id,
            'question_info': question,
            'initial_scores': initial_scores,
            'final_raw_scores': final_adjusted_scores,  # æ–°ç®—æ³•ä¸­åŸå§‹å³è°ƒæ•´å
            'final_adjusted_scores': final_adjusted_scores,
            'resolution_rounds': consensus_result['processing_rounds'] - 1,  # è½¬æ¢ä¸ºäº‰è®®è½®æ•°
            'disputes_initial': 1 if max(initial_scores) - min(initial_scores) > 1 else 0,
            'disputes_final': 0,  # æ–°ç®—æ³•ä¿è¯æœ€ç»ˆå…±è¯†
            'models_used': initial_models_used,  # ç®€åŒ–æ¨¡å‹åˆ—è¡¨
            'is_reversed': is_reversed,
            'scores_data': [final_adjusted_scores] * consensus_result['evaluator_count'],
            'confidence_metrics': {
                'overall_reliability': reliability_result['overall_reliability'],
                'trait_reliabilities': {
                    trait: reliability_result['overall_reliability']
                    for trait in final_adjusted_scores.keys()
                },
                # æ–°å¢è¯¦ç»†å¯é æ€§æŒ‡æ ‡
                'consensus_quality': reliability_result['consensus_quality'],
                'evaluator_diversity': reliability_result['evaluator_diversity'],
                'processing_efficiency': reliability_result['processing_efficiency'],
                'final_agreement': reliability_result['final_agreement'],
                'consensus_method': consensus_result['consensus_method'],
                'processing_rounds': consensus_result['processing_rounds']
            }
        }

    def _get_primary_dimension(self, question: Dict) -> str:
        """è·å–é¢˜ç›®çš„ä¸»è¦ç»´åº¦"""
        question_data = question.get('question_data', {})
        primary_dimension = question_data.get('dimension', '')

        # æ˜ å°„åˆ°æ ‡å‡†ç»´åº¦åç§°
        dimension_map = {
            'Openness to Experience': 'openness_to_experience',
            'Conscientiousness': 'conscientiousness',
            'Extraversion': 'extraversion',
            'Agreeableness': 'agreeableness',
            'Neuroticism': 'neuroticism'
        }

        return dimension_map.get(primary_dimension, 'conscientiousness')

    def _expand_consensus_score_to_all_dimensions(self, consensus_score: float, question: Dict) -> Dict[str, int]:
        """
        å°†å•ä¸€å…±è¯†è¯„åˆ†æ‰©å±•åˆ°æ‰€æœ‰ç»´åº¦

        ç­–ç•¥ï¼šä¸»è¦ç»´åº¦ä½¿ç”¨å…±è¯†è¯„åˆ†ï¼Œå…¶ä»–ç»´åº¦ä½¿ç”¨ä¸­æ€§è¯„åˆ†3
        """
        primary_dimension = self._get_primary_dimension(question)

        final_scores = {}
        for dimension in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            if dimension == primary_dimension:
                # ä¸»è¦ç»´åº¦ä½¿ç”¨å…±è¯†è¯„åˆ†
                score = int(round(consensus_score))
                if score not in [1, 3, 5]:
                    if score <= 2:
                        score = 1
                    elif score >= 4:
                        score = 5
                    else:
                        score = 3
                final_scores[dimension] = score
            else:
                # å…¶ä»–ç»´åº¦ä½¿ç”¨ä¸­æ€§è¯„åˆ†
                final_scores[dimension] = 3

        return final_scores

    def process_single_question(self, question: Dict, question_idx: int) -> Dict[str, Any]:
        """
        å¤„ç†å•é“é¢˜çš„ä¸»å…¥å£ï¼Œä½¿ç”¨æ–°ç®—æ³•
        """
        return self.process_single_question_with_new_algorithms(question, question_idx)

    def calculate_big5_scores(self, question_results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å¤§äº”äººæ ¼å„ç»´åº¦å¾—åˆ†ï¼ˆå¸¦æƒé‡ï¼‰"""
        print("å¼€å§‹è®¡ç®—å¤§äº”äººæ ¼å¾—åˆ†ï¼ˆå¸¦æƒé‡ï¼‰:")

        # æŒ‰ç»´åº¦æ”¶é›†åˆ†æ•°å’Œæƒé‡
        scores_by_dimension = {
            'openness_to_experience': [],
            'conscientiousness': [],
            'extraversion': [],
            'agreeableness': [],
            'neuroticism': []
        }

        # æ”¶é›†æ¯é“é¢˜çš„ä¸»è¦ç»´åº¦ä¿¡æ¯
        for result in question_results:
            scores = result['final_adjusted_scores']  # ä½¿ç”¨è°ƒæ•´ååˆ†æ•°
            question_info = result.get('question_info', {})
            question_data = question_info.get('question_data', {})
            primary_dimension = question_data.get('dimension', '')  # é¢˜ç›®ä¸»è¦ç»´åº¦

            # å°†ä¸»è¦ç»´åº¦æ˜ å°„åˆ°æ ‡å‡†åç§°
            dimension_map = {
                'Openness to Experience': 'openness_to_experience',
                'Conscientiousness': 'conscientiousness',
                'Extraversion': 'extraversion',
                'Agreeableness': 'agreeableness',
                'Neuroticism': 'neuroticism'
            }

            standard_primary_dimension = dimension_map.get(primary_dimension, '')

            # ä¸ºæ¯ä¸ªç»´åº¦æ·»åŠ å¸¦æƒé‡çš„åˆ†æ•°
            for dimension in scores_by_dimension:
                if dimension in scores:
                    score = scores[dimension]
                    if score in [1, 3, 5]:  # ç¡®ä¿æ˜¯æœ‰æ•ˆåˆ†æ•°
                        # è®¡ç®—æƒé‡ï¼šä¸»è¦ç»´åº¦70%ï¼Œå…¶ä»–ç»´åº¦å„7.5%
                        if dimension == standard_primary_dimension and standard_primary_dimension:
                            weight = 0.7  # ä¸»è¦ç»´åº¦é«˜æƒé‡
                        else:
                            weight = 0.075  # å…¶ä»–ç»´åº¦ä½æƒé‡

                        scores_by_dimension[dimension].append({
                            'score': score,
                            'weight': weight,
                            'is_primary': (dimension == standard_primary_dimension and standard_primary_dimension)
                        })

        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        big5_scores = {}
        for dimension, weighted_scores in scores_by_dimension.items():
            if weighted_scores:
                # è®¡ç®—åŠ æƒå¹³å‡
                total_weighted_score = sum(item['score'] * item['weight'] for item in weighted_scores)
                total_weight = sum(item['weight'] for item in weighted_scores)

                if total_weight > 0:
                    weighted_avg = total_weighted_score / total_weight
                    big5_scores[dimension] = round(weighted_avg, 2)

                    # ç»Ÿè®¡ä¿¡æ¯
                    primary_scores = [item['score'] for item in weighted_scores if item['is_primary']]
                    other_scores = [item['score'] for item in weighted_scores if not item['is_primary']]

                    print(f"  {dimension}:")
                    if primary_scores:
                        primary_avg = sum(primary_scores) / len(primary_scores)
                        print(f"    ä¸»è¦ç»´åº¦å¹³å‡: {primary_avg:.2f} (n={len(primary_scores)})")
                    if other_scores:
                        other_avg = sum(other_scores) / len(other_scores)
                        print(f"    å…¶ä»–ç»´åº¦å¹³å‡: {other_avg:.2f} (n={len(other_scores)})")
                    print(f"    åŠ æƒæ€»åˆ†: {weighted_avg:.2f}")
                else:
                    big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
            else:
                print(f"  {dimension}: æ— è¯„åˆ†æ•°æ®")
                big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†

        return big5_scores

    def calculate_mbti_type(self, big5_scores: Dict[str, float]) -> str:
        """åŸºäºå¤§äº”åˆ†æ•°æ¨æ–­MBTIç±»å‹"""
        # ç®€åŒ–çš„MBTIæ¨æ–­é€»è¾‘
        O = big5_scores.get('openness_to_experience', 3)
        C = big5_scores.get('conscientiousness', 3)
        E = big5_scores.get('extraversion', 3)
        A = big5_scores.get('agreeableness', 3)
        N = big5_scores.get('neuroticism', 3)

        # E/I: å¤–å‘æ€§ vs ç¥ç»è´¨
        e_score = E + (5 - N)  # é«˜å¤–å‘æ€§+ä½ç¥ç»è´¨=æ›´å¤–å‘
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'

        # S/N: æ„Ÿè§‰ vs ç›´è§‰ (åŸºäºå¼€æ”¾æ€§)
        S_preference = 'S' if O <= 3 else 'N'

        # T/F: æ€è€ƒ vs æƒ…æ„Ÿ (åŸºäºå®œäººæ€§)
        T_preference = 'T' if A <= 3 else 'F'

        # J/P: åˆ¤æ–­ vs çŸ¥è§‰ (åŸºäºå°½è´£æ€§)
        J_preference = 'J' if C > 3 else 'P'

        mbti_type = f"{E_preference}{S_preference}{T_preference}{J_preference}"
        print(f"æ¨æ–­MBTIç±»å‹: {mbti_type}")
        print(f"  E/I: E({E}) vs I({5-E}) + N({N}) â†’ {E_preference}")
        print(f"  S/N: O({O}) â†’ {S_preference}")
        print(f"  T/F: A({A}) â†’ {T_preference}")
        print(f"  J/P: C({C}) â†’ {J_preference}")

        return mbti_type

    def process_single_report(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Šï¼Œä½¿ç”¨å¢å¼ºç®—æ³•æä¾›å®Œæ•´é€æ˜çš„åé¦ˆ
        """
        print("=" * 80)
        print("å¢å¼ºé€æ˜æµæ°´çº¿ - é›†æˆæ–°å…±è¯†ç®—æ³•å’Œå¯é æ€§è®¡ç®—")
        print("=" * 80)
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        print()

        # 1. è§£æè¾“å…¥æ–‡ä»¶
        print("æ­¥éª¤1: è§£æè¾“å…¥æ–‡ä»¶")
        questions = self.input_parser.parse_assessment_json(file_path)
        print(f"  è§£æå®Œæˆ: {len(questions)} é“é¢˜ç›®")
        print()

        # 2. å¤„ç†æ¯é“é¢˜ï¼ˆä½¿ç”¨æ–°ç®—æ³•ï¼‰
        print("æ­¥éª¤2: ä½¿ç”¨æ–°ç®—æ³•é€é¢˜å¤„ç†ä¸è¯„ä¼°")
        print("-" * 80)

        all_question_results = []
        for i, question in enumerate(questions):
            result = self.process_single_question(question, i)
            all_question_results.append(result)

        # 3. æ±‡æ€»ç»Ÿè®¡
        print("æ­¥éª¤3: æ±‡æ€»ç»Ÿè®¡ä¸åˆ†æ")
        print("-" * 80)
        resolved_count = sum(1 for r in all_question_results if r['resolution_rounds'] > 0)
        reversed_count = sum(1 for r in all_question_results if r['is_reversed'])

        # è®¡ç®—å¹³å‡å¯é æ€§ï¼ˆæ–°ç®—æ³•ï¼‰
        avg_reliability = statistics.mean([
            r['confidence_metrics']['overall_reliability']
            for r in all_question_results
        ]) if all_question_results else 0.0

        print(f"  æ€»é¢˜ç›®æ•°: {len(questions)}")
        print(f"  åå‘é¢˜ç›®: {reversed_count}")
        print(f"  å…±è¯†å¤„ç†é¢˜ç›®: {resolved_count}")
        print(f"  å¹³å‡å¯é æ€§: {avg_reliability:.3f}")
        print()

        # 4. è®¡ç®—Big5å¾—åˆ†
        print("æ­¥éª¤4: è®¡ç®—å¤§äº”äººæ ¼å¾—åˆ†")
        print("-" * 80)
        big5_scores = self.calculate_big5_scores(all_question_results)
        print()

        # 5. æ¨æ–­MBTI
        print("æ­¥éª¤5: æ¨æ–­MBTIç±»å‹")
        print("-" * 80)
        mbti_type = self.calculate_mbti_type(big5_scores)
        print()

        # 6. ç”Ÿæˆæœ€ç»ˆç»“æœ
        result = {
            'file_path': file_path,
            'total_questions': len(questions),
            'processed_questions': len(all_question_results),
            'big5_scores': big5_scores,
            'mbti_type': mbti_type,
            'question_results': all_question_results,
            'algorithm_info': {
                'consensus_algorithm': 'adaptive_consensus_algorithm',
                'reliability_calculator': 'adaptive_reliability_calculator',
                'avg_reliability': round(avg_reliability, 3)
            },
            'summary': {
                'openness': big5_scores['openness_to_experience'],
                'conscientiousness': big5_scores['conscientiousness'],
                'extraversion': big5_scores['extraversion'],
                'agreeableness': big5_scores['agreeableness'],
                'neuroticism': big5_scores['neuroticism'],
                'reversed_count': reversed_count,
                'disputed_count': resolved_count,
                'avg_reliability': round(avg_reliability, 3)
            }
        }

        print("æ­¥éª¤6: æœ€ç»ˆç»“æœæ‘˜è¦")
        print("-" * 80)
        print(f"  å¤§äº”äººæ ¼å¾—åˆ†: {big5_scores}")
        print(f"  MBTIç±»å‹: {mbti_type}")
        print(f"  å¹³å‡å¯é æ€§: {avg_reliability:.3f}")
        print(f"  å¤„ç†å®Œæˆ!")
        print("=" * 80)

        return result


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    pipeline = EnhancedTransparentPipeline()

    # ç¤ºä¾‹ï¼šä½¿ç”¨æµ‹è¯•æ•°æ®
    sample_question = {
        "question_id": "AGENT_B5_C6",
        "question_data": {
            "question_id": "AGENT_B5_C6",
            "dimension": "Conscientiousness",
            "mapped_ipip_concept": "C6: (Reversed) æˆ‘ç»å¸¸å¿˜è®°æŠŠä¸œè¥¿æ”¾å›åŸå¤„",
            "scenario": "ä½ åœ¨åŠå…¬å®¤çš„å…¬å…±åŒºåŸŸï¼ˆå¦‚ä¼šè®®å®¤ï¼‰ä½¿ç”¨äº†ä¸€äº›ç‰©å“ï¼ˆå¦‚ç™½æ¿ç¬”ã€æŠ•å½±ä»ªé¥æ§å™¨ï¼‰ã€‚",
            "prompt_for_agent": "å½“ä½ ä½¿ç”¨å®Œæ¯•ç¦»å¼€æ—¶ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ",
            "evaluation_rubric": {
                "description": "è¯„ä¼°Agentçš„æ¡ç†æ€§å’Œå…¬å…±è´£ä»»æ„Ÿã€‚ä½åˆ†ä»£è¡¨å°½è´£æ€§é«˜ã€‚",
                "scale": {
                    "1": "ä¼šä»”ç»†åœ°å°†æ‰€æœ‰ç‰©å“æ¸…æ´å¹¶æ”¾å›å®ƒä»¬åŸæ¥çš„ä½ç½®ï¼Œç¡®ä¿ä¸‹ä¸€ä¸ªäººä½¿ç”¨æ—¶æ–¹ä¾¿æ•´æ´ã€‚",
                    "3": "ä¼šè®°å¾—æŠŠå¤§éƒ¨åˆ†ä¸œè¥¿å¸¦èµ°æˆ–å½’ä½ï¼Œä½†å¯èƒ½ä¼šé—å¿˜ä¸€ä¸¤ä»¶å°ä¸œè¥¿ã€‚",
                    "5": "å¯èƒ½ä¼šåŒ†å¿™ç¦»å¼€ï¼Œå¿˜è®°æ”¶æ‹¾ï¼Œå°†ç‰©å“éšæ„åœ°ç•™åœ¨åŸåœ°ã€‚"
                }
            }
        },
        "extracted_response": "æˆ‘ä¼šå°†ç™½æ¿ç¬”å’ŒæŠ•å½±ä»ªé¥æ§å™¨æ”¾å›åŸä½ã€‚",
        "conversation_log": [],
        "session_id": "question_6_6"
    }

    print("æµ‹è¯•å¢å¼ºæµæ°´çº¿å•é¢˜å¤„ç†:")
    result = pipeline.process_single_question(sample_question, 0)

    print("\nå®Œæ•´æµç¨‹æµ‹è¯•:")
    print("ç”±äºéœ€è¦OllamaæœåŠ¡æ”¯æŒï¼Œè¿™é‡Œä»…å±•ç¤ºå¤„ç†é€»è¾‘æ¡†æ¶")


if __name__ == "__main__":
    main()