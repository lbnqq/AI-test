#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„é€æ˜æµæ°´çº¿ - åŸºäºTDDçš„ç»´åº¦å¤„ç†æ”¹è¿›
Phase 1: ä¸»ç»´åº¦ä¿ç•™çœŸå®å¹³å‡åˆ†ï¼ˆ4.33è€Œä¸æ˜¯5ï¼‰
Phase 2: æ¬¡ç»´åº¦ä½¿ç”¨è®¡ç®—å‡åˆ†ï¼ˆè€Œä¸æ˜¯å›ºå®š3åˆ†ï¼‰
æƒé‡åˆ†é…ä¿æŒä¸å˜ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
"""

import json
import ollama
from typing import Dict, List, Any
from .context_generator import ContextGenerator
from .reverse_scoring_processor import ReverseScoringProcessor
from .input_parser import InputParser
import time
import statistics
import re

# å¯¼å…¥åŸæœ‰ç®—æ³•ç»„ä»¶
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from adaptive_consensus_algorithm import AdaptiveConsensusAlgorithm
from adaptive_reliability_calculator import AdaptiveReliabilityCalculator


class ImprovedTransparentPipeline:
    """
    æ”¹è¿›çš„é€æ˜æµæ°´çº¿

    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä¸»ç»´åº¦ä¿ç•™çœŸå®å¹³å‡åˆ†ï¼ˆä¸å–æ•´ä¸º1,3,5ï¼‰
    2. æ¬¡ç»´åº¦è®¡ç®—çœŸå®å‡åˆ†ï¼ˆä¸å›ºå®šç»™3åˆ†ï¼‰
    3. æƒé‡åˆ†é…ä¿æŒä¸å˜ï¼ˆç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ï¼‰
    4. äº‘ç«¯ä¼˜å…ˆï¼Œæœ¬åœ°å¤‡ä»½
    """

    def __init__(self, primary_models: List[str] = None, dispute_models: List[str] = None, use_cloud: bool = True, preserve_precision: bool = True):
        """
        åˆå§‹åŒ–æ”¹è¿›æµæ°´çº¿

        Args:
            primary_models: ä¸»è¦è¯„ä¼°æ¨¡å‹åˆ—è¡¨
            dispute_models: äº‰è®®è§£å†³æ¨¡å‹åˆ—è¡¨
            use_cloud: æ˜¯å¦ä½¿ç”¨äº‘ç«¯æ¨¡å‹
            preserve_precision: æ˜¯å¦ä¿ç•™ç²¾åº¦ï¼ˆTDDæ”¹è¿›å¼€å…³ï¼‰
        """
        self.use_cloud = use_cloud
        self.preserve_precision = preserve_precision  # æ–°å¢TDDæ”¹è¿›å¼€å…³

        if use_cloud:
            # äº‘ç«¯ä¼˜å…ˆé…ç½®
            self.primary_models = primary_models or [
                'deepseek-v3.1:671b-cloud',  # 671Bå‚æ•°ï¼Œä¸»åŠ›æ¨¡å‹
                'gpt-oss:120b-cloud',       # 120Bå‚æ•°ï¼Œç‹¬ç«‹éªŒè¯
                'qwen3-vl:235b-cloud'       # 235Bå‚æ•°ï¼Œé«˜è´¨é‡è¡¥å……
            ]

            self.dispute_models = dispute_models or [
                'qwen3-vl:235b-cloud',       # é«˜è´¨é‡äº‰è®®è§£å†³
                'gpt-oss:120b-cloud',       # æœ€ç»ˆä»²è£
                'qwen3:8b',                  # æœ¬åœ°å¤‡ä»½1
                'deepseek-r1:8b'            # æœ¬åœ°å¤‡ä»½2
            ]
        else:
            # æœ¬åœ°æ¨¡å‹é…ç½®
            self.primary_models = primary_models or [
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral-nemo:latest'
            ]

            self.dispute_models = dispute_models or [
                'llama3:latest',      # Meta (ç¬¬1è½®ç¬¬1ä¸ª)
                'gemma3:latest',      # Google (ç¬¬1è½®ç¬¬2ä¸ª)
                'phi3:mini',          # Microsoft (ç¬¬2è½®ç¬¬1ä¸ª)
                'yi:6b',              # 01.AI (ç¬¬2è½®ç¬¬2ä¸ª)
                'qwen3:4b',           # Alibaba (ç¬¬3è½®ç¬¬1ä¸ª)
                'deepseek-r1:8b',     # DeepSeek (ç¬¬3è½®ç¬¬2ä¸ª)
                'mixtral:8x7b'        # Mistral (å¤‡ç”¨)
            ]

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.context_generator = ContextGenerator()
        self.reverse_processor = ReverseScoringProcessor()
        self.input_parser = InputParser()
        self.adaptive_consensus = AdaptiveConsensusAlgorithm()
        self.adaptive_reliability = AdaptiveReliabilityCalculator()

    def evaluate_single_question(self, context: str, model: str, question_id: str) -> Dict[str, int]:
        """
        ä½¿ç”¨å•ä¸ªæ¨¡å‹è¯„ä¼°å•é“é¢˜ï¼Œæä¾›æ™ºèƒ½å›é€€ï¼Œç»å¯¹ç¦æ­¢é»˜è®¤è¯„åˆ†
        """
        print(f"    â””â”€ ä½¿ç”¨æ¨¡å‹ {model} è¯„ä¼°é¢˜ç›® {question_id}...")

        # å®šä¹‰å›é€€æ¨¡å‹åˆ—è¡¨ï¼ˆäº‘ç«¯ä¼˜å…ˆï¼Œæœ¬åœ°å¤‡ä»½ï¼‰
        if model.endswith('-cloud'):
            fallback_models = [
                model,  # é¦–é€‰äº‘ç«¯æ¨¡å‹
                # å…¶ä»–äº‘ç«¯æ¨¡å‹
                'gpt-oss:120b-cloud',
                'qwen3-vl:235b-cloud',
                # æœ¬åœ°æ¨¡å‹
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral:instruct'
            ]
        else:
            fallback_models = [
                model,  # é¦–é€‰æœ¬åœ°æ¨¡å‹
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral:instruct'
            ]

        last_error = None

        for attempt_model in fallback_models:
            try:
                print(f"      å°è¯•ä½¿ç”¨æ¨¡å‹: {attempt_model}")

                # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
                if attempt_model.endswith('-cloud'):
                    time.sleep(2)
                else:
                    time.sleep(0.5)

                response = ollama.generate(model=attempt_model, prompt=context, options={'num_predict': 2000})
                scores = self.parse_scores_from_response(response['response'])

                # éªŒè¯è¯„åˆ†æœ‰æ•ˆæ€§
                if self._validate_scores(scores):
                    print(f"      âœ… è¯„åˆ†æˆåŠŸ: {scores}")
                    return scores
                else:
                    print(f"      âš ï¸ è¯„åˆ†æ— æ•ˆ: {scores}")
                    continue

            except Exception as e:
                last_error = e
                print(f"      âŒ æ¨¡å‹ {attempt_model} å¤±è´¥: {str(e)}")
                continue

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥æ—¶çš„é”™è¯¯å¤„ç†
        error_msg = f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}"
        print(f"      ğŸš¨ {error_msg}")

        # ç»å¯¹ç¦æ­¢è¿”å›é»˜è®¤è¯„åˆ†ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise RuntimeError(error_msg)

    def parse_scores_from_response(self, response: str) -> Dict[str, int]:
        """ä»æ¨¡å‹å“åº”ä¸­è§£æBig Fiveè¯„åˆ†"""
        scores = {}

        # å°è¯•å¤šç§è§£ææ¨¡å¼
        patterns = [
            r'openness[_\s]*to[_\s]*experience[:\s]*([1-5])',
            r'conscientiousness[:\s]*([1-5])',
            r'extraversion[:\s]*([1-5])',
            r'agreeableness[:\s]*([1-5])',
            r'neuroticism[:\s]*([1-5])',
            # çŸ­æ ¼å¼
            r'O[:\s]*([1-5])',
            r'C[:\s]*([1-5])',
            r'E[:\s]*([1-5])',
            r'A[:\s]*([1-5])',
            r'N[:\s]*([1-5])',
            # å¸¦å†’å·çš„æ ¼å¼
            r'openness[:\s]*to[_\s]*experience[:\s]*:\s*([1-5])',
            r'conscientiousness[:\s]*:\s*([1-5])',
            r'extraversion[:\s]*:\s*([1-5])',
            r'agreeableness[:\s]*:\s*([1-5])',
            r'neuroticism[:\s]*:\s*([1-5])',
        ]

        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                score = int(match.group(1))
                dimension = self._pattern_to_dimension(pattern)
                if dimension:
                    scores[dimension] = score
                    break  # æ‰¾åˆ°åŒ¹é…å°±åœæ­¢

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†ï¼Œå°è¯•ä»æ•°å­—ä¸­æå–
        if not scores:
            # å¯»æ‰¾1-5çš„æ•°å­—
            numbers = re.findall(r'\b([1-5])\b', response)
            if numbers:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ•°å­—ä½œä¸ºé»˜è®¤è¯„åˆ†
                default_score = int(numbers[0])
                scores = {
                    'openness_to_experience': default_score,
                    'conscientiousness': default_score,
                    'extraversion': default_score,
                    'agreeableness': default_score,
                    'neuroticism': default_score
                }

        return scores

    def _pattern_to_dimension(self, pattern: str) -> str:
        """å°†æ­£åˆ™æ¨¡å¼æ˜ å°„åˆ°ç»´åº¦åç§°"""
        if 'openness' in pattern:
            return 'openness_to_experience'
        elif 'conscientious' in pattern:
            return 'conscientiousness'
        elif 'extraversion' in pattern or r'\bE\b' in pattern:
            return 'extraversion'
        elif 'agreeableness' in pattern or r'\bA\b' in pattern:
            return 'agreeableness'
        elif 'neuroticism' in pattern or r'\bN\b' in pattern:
            return 'neuroticism'
        return None

    def _validate_scores(self, scores: Dict[str, int]) -> bool:
        """éªŒè¯è¯„åˆ†çš„æœ‰æ•ˆæ€§"""
        if not scores:
            return False

        required_dimensions = [
            'openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'
        ]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€ç»´åº¦
        for dimension in required_dimensions:
            if dimension not in scores:
                return False
            if not isinstance(scores[dimension], int):
                return False
            if scores[dimension] < 1 or scores[dimension] > 5:
                return False

        return True

    def process_single_question(self, question: Dict, question_idx: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ”¹è¿›ç®—æ³•å¤„ç†å•ä¸ªé—®é¢˜
        """
        question_id = question.get('question_id', f"q{question_idx}")
        question_data = question.get('question_data', {})

        print(f"å¤„ç†ç¬¬ {question_idx + 1} é¢˜ (ID: {question_id}) - ä½¿ç”¨æ”¹è¿›ç®—æ³•")

        # ç”Ÿæˆè¯„ä¼°ä¸Šä¸‹æ–‡
        context = self.context_generator.generate_context(question_data)

        # è·å–é¢˜ç›®ä¸»è¦ç»´åº¦
        primary_dimension = self._get_primary_dimension(question)
        is_reversed = self._get_is_reversed(question_data)

        print(f"  é¢˜ç›®æ¦‚å¿µ: {question_data.get('mapped_ipip_concept', 'Unknown')}")
        print(f"  æ˜¯å¦åå‘: {is_reversed}")
        print(f"  è¢«è¯•å›ç­”: {question.get('answer', 'No answer provided')[:100]}...")

        # ä½¿ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•è¿›è¡Œè¯„ä¼°
        print(f"  ä½¿ç”¨æ”¹è¿›çš„é€‚åº”æ€§å…±è¯†ç®—æ³•è¿›è¡Œè¯„ä¼°:")

        # Phase 1: è·å–å…±è¯†è¯„åˆ†
        consensus_result = self._get_adaptive_consensus(context, question, question_id)
        consensus_score = consensus_result['consensus_score']  # ä¿ç•™çœŸå®å¹³å‡åˆ†

        # Phase 2: æ‰©å±•åˆ°æ‰€æœ‰ç»´åº¦ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        final_adjusted_scores = self._expand_consensus_score_to_all_dimensions_improved(
            consensus_score, question, context
        )

        # åº”ç”¨åå‘è®¡åˆ†è½¬æ¢
        if is_reversed:
            final_adjusted_scores = {
                trait: self.reverse_processor.reverse_score(score)
                for trait, score in final_adjusted_scores.items()
            }
            print(f"  åº”ç”¨åå‘è®¡åˆ†è½¬æ¢: {final_adjusted_scores}")

        # è®¡ç®—å¯é æ€§
        reliability_result = self.adaptive_reliability.calculate_adaptive_reliability(
            consensus_result['final_scores'],
            consensus_result['evaluator_count'],
            consensus_result['processing_rounds'],
            consensus_result['consensus_method']
        )

        print(f"  æœ€ç»ˆè¯„åˆ†: {final_adjusted_scores}")
        print(f"  æ€»ä½“å¯é æ€§: {reliability_result['overall_reliability']:.3f}")
        print()

        return {
            'question_id': question_id,
            'question_info': question,
            'initial_scores': consensus_result['final_scores'],
            'final_raw_scores': final_adjusted_scores,
            'final_adjusted_scores': final_adjusted_scores,
            'resolution_rounds': consensus_result['processing_rounds'] - 1,
            'disputes_initial': 1 if max(consensus_result['final_scores']) - min(consensus_result['final_scores']) > 1 else 0,
            'disputes_final': 0,  # æ”¹è¿›ç®—æ³•ä¿è¯æœ€ç»ˆå…±è¯†
            'models_used': consensus_result['evaluator_count'],
            'is_reversed': is_reversed,
            'scores_data': [final_adjusted_scores] * consensus_result['evaluator_count'],
            'confidence_metrics': {
                'overall_reliability': reliability_result['overall_reliability'],
                'trait_reliabilities': {
                    trait: reliability_result['overall_reliability']
                    for trait in final_adjusted_scores.keys()
                },
                # è¯¦ç»†å¯é æ€§æŒ‡æ ‡
                'consensus_quality': reliability_result['consensus_quality'],
                'evaluator_diversity': reliability_result['evaluator_diversity'],
                'processing_efficiency': reliability_result['processing_efficiency'],
                'final_agreement': reliability_result['final_agreement'],
                'consensus_method': consensus_result['consensus_method'],
                'processing_rounds': consensus_result['processing_rounds']
            }
        }

    def _get_adaptive_consensus(self, context: str, question: Dict, question_id: str) -> Dict[str, Any]:
        """è·å–é€‚åº”æ€§å…±è¯†è¯„åˆ†"""
        initial_scores = []
        initial_models_used = []

        print(f"  è·å–åˆå§‹3ä¸ªè¯„ä¼°å™¨è¯„åˆ†:")

        for i in range(3):
            model = self.primary_models[i % len(self.primary_models)]
            try:
                scores = self.evaluate_single_question(context, model, f"{question_id}_init_{i}")

                # ä½¿ç”¨é¢˜ç›®ä¸»è¦ç»´åº¦ä½œä¸ºå•ä¸€è¯„åˆ†
                primary_dimension = self._get_primary_dimension(question)
                single_score = scores.get(primary_dimension, 3)

                # ç¡®ä¿è¯„åˆ†æ˜¯1,3,5
                if single_score not in [1, 3, 5]:
                    if single_score <= 2:
                        single_score = 1
                    elif single_score >= 4:
                        single_score = 5
                    else:
                        single_score = 3

                print(f"    âœ… {model}: {single_score}")
                initial_scores.append(single_score)
                initial_models_used.append(model)

            except Exception as e:
                print(f"    âŒ {model}: å¤±è´¥ - {e}")
                # ä½¿ç”¨å¤‡ç”¨è¯„åˆ†ç¡®ä¿ä¸ä¸­æ–­
                initial_scores.append(3)  # ä¸­æ€§è¯„åˆ†
                initial_models_used.append(model)

        # ä½¿ç”¨é€‚åº”æ€§å…±è¯†ç®—æ³•
        consensus_result = self.adaptive_consensus.adaptive_consensus(
            initial_scores,
            lambda needed_count: self._get_additional_scores(context, question, needed_count, question_id)
        )

        return consensus_result

    def _get_additional_scores(self, context: str, question: Dict, needed_count: int, question_id: str) -> List[int]:
        """è·å–é¢å¤–çš„è¯„ä¼°å™¨è¯„åˆ†"""
        additional_scores = []

        for i in range(needed_count):
            model_index = i % len(self.dispute_models)
            model = self.dispute_models[model_index]

            try:
                scores = self.evaluate_single_question(context, model, f"{question_id}_adaptive_{i}")

                # ä½¿ç”¨é¢˜ç›®ä¸»è¦ç»´åº¦ä½œä¸ºå•ä¸€è¯„åˆ†
                primary_dimension = self._get_primary_dimension(question)
                single_score = scores.get(primary_dimension, 3)

                # ç¡®ä¿è¯„åˆ†æ˜¯1,3,5
                if single_score not in [1, 3, 5]:
                    if single_score <= 2:
                        single_score = 1
                    elif single_score >= 4:
                        single_score = 5
                    else:
                        single_score = 3

                print(f"    âœ… {model}: {single_score}")
                additional_scores.append(single_score)

            except Exception as e:
                print(f"    âŒ {model}: å¤±è´¥ - {e}")
                # ä½¿ç”¨å¤‡ç”¨è¯„åˆ†ç¡®ä¿ä¸ä¸­æ–­
                additional_scores.append(3)  # ä¸­æ€§è¯„åˆ†

        return additional_scores

    def _get_primary_dimension(self, question: Dict) -> str:
        """è·å–é¢˜ç›®çš„ä¸»è¦ç»´åº¦"""
        question_data = question.get('question_data', {})
        primary_dimension = question_data.get('dimension', '')

        # æ˜ å°„åˆ°æ ‡å‡†ç»´åº¦åç§°
        dimension_map = {
            'Openness to Experience': 'openness_to_experience',
            'Conscientiousness': 'conscientiousness',
            'Extraversion': 'extraversion',
            'Agreeableness': 'agreeableness',
            'Neuroticism': 'neuroticism'
        }

        return dimension_map.get(primary_dimension, primary_dimension)

    def _get_is_reversed(self, question_data: Dict) -> bool:
        """æ£€æŸ¥é¢˜ç›®æ˜¯å¦éœ€è¦åå‘è®¡åˆ†"""
        concept = question_data.get('mapped_ipip_concept', '')
        return '(Reversed)' in concept

    def _expand_consensus_score_to_all_dimensions_improved(self, consensus_score: float, question: Dict, context: str) -> Dict[str, float]:
        """
        å°†å…±è¯†è¯„åˆ†æ‰©å±•åˆ°æ‰€æœ‰ç»´åº¦ï¼ˆæ”¹è¿›ç‰ˆï¼‰

        ç­–ç•¥ï¼š
        - ä¸»ç»´åº¦ï¼šä¿ç•™çœŸå®å¹³å‡åˆ†ï¼ˆTDD Phase 1æ”¹è¿›ï¼‰
        - æ¬¡ç»´åº¦ï¼šè®¡ç®—çœŸå®å‡åˆ†ï¼ˆTDD Phase 2æ”¹è¿›ï¼‰
        - æƒé‡åˆ†é…ï¼šä¿æŒä¸å˜ï¼ˆç¡®ä¿ç³»ç»Ÿç¨³å®šï¼‰
        """
        if not self.preserve_precision:
            # å¦‚æœå…³é—­ç²¾åº¦ä¿ç•™ï¼Œä½¿ç”¨åŸç®—æ³•
            return self._expand_consensus_score_to_all_dimensions_original(consensus_score, question)

        primary_dimension = self._get_primary_dimension(question)
        standard_primary_dimension = self._map_dimension_name(primary_dimension)

        # è·å–æ‰€æœ‰æ¨¡å‹çš„å®Œæ•´è¯„åˆ†ï¼ˆç”¨äºè®¡ç®—æ¬¡ç»´åº¦å‡åˆ†ï¼‰
        all_model_scores = self._get_all_model_scores(context, question)

        final_scores = {}
        for dimension in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            if dimension == standard_primary_dimension:
                # ä¸»ç»´åº¦ï¼šä¿ç•™çœŸå®å¹³å‡åˆ†ï¼ˆTDD Phase 1ï¼‰
                final_scores[dimension] = float(consensus_score)  # ä¿ç•™å°æ•°ç²¾åº¦
            else:
                # æ¬¡ç»´åº¦ï¼šè®¡ç®—çœŸå®å‡åˆ†ï¼ˆTDD Phase 2ï¼‰
                if all_model_scores:
                    scores = [model[dimension] for model in all_model_scores]
                    final_scores[dimension] = statistics.mean(scores)
                else:
                    # å¦‚æœæ²¡æœ‰æ¨¡å‹è¯„åˆ†æ•°æ®ï¼Œä½¿ç”¨ä¸­æ€§åˆ†
                    final_scores[dimension] = 3.0

        return final_scores

    def _expand_consensus_score_to_all_dimensions_original(self, consensus_score: float, question: Dict) -> Dict[str, int]:
        """åŸå§‹çš„æ‰©å±•æ–¹æ³•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        primary_dimension = self._get_primary_dimension(question)

        final_scores = {}
        for dimension in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            if dimension == primary_dimension:
                # ä¸»è¦ç»´åº¦ä½¿ç”¨å…±è¯†è¯„åˆ†
                score = int(round(consensus_score))
                if score not in [1, 3, 5]:
                    if score <= 2:
                        score = 1
                    elif score >= 4:
                        score = 5
                    else:
                        score = 3
                final_scores[dimension] = score
            else:
                # å…¶ä»–ç»´åº¦ä½¿ç”¨ä¸­æ€§è¯„åˆ†
                final_scores[dimension] = 3

        return final_scores

    def _map_dimension_name(self, dimension_name: str) -> str:
        """æ˜ å°„ç»´åº¦åç§°"""
        dimension_map = {
            'Openness to Experience': 'openness_to_experience',
            'Conscientiousness': 'conscientiousness',
            'Extraversion': 'extraversion',
            'Agreeableness': 'agreeableness',
            'Neuroticism': 'neuroticism'
        }
        return dimension_map.get(dimension_name, dimension_name)

    def _get_all_model_scores(self, context: str, question: Dict) -> List[Dict[str, int]]:
        """
        è·å–æ‰€æœ‰æ¨¡å‹çš„å®Œæ•´è¯„åˆ†ï¼ˆç”¨äºè®¡ç®—æ¬¡ç»´åº¦å‡åˆ†ï¼‰

        TDD Phase 3: å®ç°æ¬¡ç»´åº¦çœŸå®å‡åˆ†è®¡ç®—çš„æ•°æ®æ”¶é›†
        """
        if not hasattr(self, 'individual_model_scores') or not self.individual_model_scores:
            # å¦‚æœæ²¡æœ‰å­˜å‚¨çš„æ¨¡å‹è¯„åˆ†ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆä½¿ç”¨é»˜è®¤ä¸­æ€§åˆ†ï¼‰
            return []

        # ä»å­˜å‚¨çš„æ¨¡å‹è¯„åˆ†ä¸­æå–å½“å‰é¢˜ç›®çš„æ‰€æœ‰æ¨¡å‹è¯„åˆ†
        question_id = question.get('question_id', '')
        model_scores = []

        # éå†æ‰€æœ‰å­˜å‚¨çš„æ¨¡å‹è¯„åˆ†
        for model_score_list in self.individual_model_scores:
            for score_data in model_score_list:
                if score_data.get('question_id') == question_id:
                    # æå–è¯¥æ¨¡å‹çš„Big Fiveè¯„åˆ†
                    scores = score_data.get('big5_scores', {})
                    if scores:
                        model_scores.append(scores)

        return model_scores

    def calculate_big5_scores(self, question_results: List[Dict]) -> Dict[str, float]:
        """
        è®¡ç®—æœ€ç»ˆçš„Big Fiveè¯„åˆ†ï¼ˆä½¿ç”¨æƒé‡åˆ†é…ï¼‰

        æƒé‡åˆ†é…ä¿æŒä¸å˜ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ï¼š
        - ä¸»ç»´åº¦ï¼š70%æƒé‡
        - æ¬¡ç»´åº¦ï¼šå„7.5%æƒé‡
        """
        scores_by_dimension = {
            'openness_to_experience': [],
            'conscientiousness': [],
            'extraversion': [],
            'agreeableness': [],
            'neuroticism': []
        }

        # æ”¶é›†æ¯é“é¢˜çš„ä¸»è¦ç»´åº¦ä¿¡æ¯
        for result in question_results:
            scores = result['final_adjusted_scores']
            question_info = result.get('question_info', {})
            question_data = question_info.get('question_data', {})
            primary_dimension = question_data.get('dimension', '')

            # å°†ä¸»è¦ç»´åº¦æ˜ å°„åˆ°æ ‡å‡†åç§°
            dimension_map = {
                'Openness to Experience': 'openness_to_experience',
                'Conscientiousness': 'conscientiousness',
                'Extraversion': 'extraversion',
                'Agreeableness': 'agreeableness',
                'Neuroticism': 'neuroticism'
            }
            standard_primary_dimension = dimension_map.get(primary_dimension, '')

            # ä¸ºæ¯ä¸ªç»´åº¦æ·»åŠ å¸¦æƒé‡çš„åˆ†æ•°
            for dimension in scores_by_dimension:
                if dimension in scores:
                    score = scores[dimension]
                    if isinstance(score, (int, float)):
                        # è®¡ç®—æƒé‡ï¼šä¸»è¦ç»´åº¦70%ï¼Œå…¶ä»–ç»´åº¦å„7.5%
                        if dimension == standard_primary_dimension and standard_primary_dimension:
                            weight = 0.7  # ä¸»è¦ç»´åº¦é«˜æƒé‡
                        else:
                            weight = 0.075  # å…¶ä»–ç»´åº¦ä½æƒé‡

                        scores_by_dimension[dimension].append({
                            'score': float(score),
                            'weight': weight,
                            'is_primary': (dimension == standard_primary_dimension and standard_primary_dimension)
                        })

        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        big5_scores = {}
        total_weight = 0

        for dimension, weighted_scores in scores_by_dimension.items():
            if weighted_scores:
                # è®¡ç®—åŠ æƒå¹³å‡
                weighted_sum = sum(item['score'] * item['weight'] for item in weighted_scores)
                total_weight_sum = sum(item['weight'] for item in weighted_scores)

                if total_weight_sum > 0:
                    big5_scores[dimension] = weighted_sum / total_weight_sum
                    total_weight += total_weight_sum
                else:
                    big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
            else:
                print(f"  {dimension}: æ— è¯„åˆ†æ•°æ®")
                big5_scores[dimension] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†

        # æ ‡å‡†åŒ–åˆ°0-5åˆ†
        for dimension in big5_scores:
            big5_scores[dimension] = max(1.0, min(5.0, big5_scores[dimension]))

        # è®¡ç®—MBTIç±»å‹
        mbti_type = self._calculate_mbti_type(big5_scores)

        return big5_scores

    def _calculate_mbti_type(self, big5_scores: Dict[str, float]) -> str:
        """æ ¹æ®Big Fiveè¯„åˆ†è®¡ç®—MBTIç±»å‹"""
        # ç®€åŒ–çš„MBTIè®¡ç®—é€»è¾‘
        e_score = big5_scores.get('extraversion', 3.0)
        i_score = big5_scores.get('openness_to_experience', 3.0)
        s_score = big5_scores.get('conscientiousness', 3.0)
        t_score = big5_scores.get('agreeableness', 3.0)
        f_score = big5_scores.get('neuroticism', 3.0)

        # Iç»´åº¦ï¼šå†…å‘å¤–å‘
        i_type = 'I' if e_score < 2.5 else 'E'

        # Nç»´åº¦ï¼šç›´è§‰vsæ€è€ƒ
        n_type = 'N' if f_score < 2.5 else 'S'

        # Tç»´åº¦ï¼šæ€è€ƒvsæƒ…æ„Ÿ
        t_type = 'T' if t_score > 3.5 else 'F'

        # Jç»´åº¦ï¼šåˆ¤æ–­vsæ„ŸçŸ¥
        j_type = 'J' if f_score > 3.5 else 'P'

        # Pç»´åº¦ï¼šæ„ŸçŸ¥vsè®¡åˆ’
        p_type = 'P' if s_score > 3.5 else 'J'

        return f"{i_type}{n_type}{t_type}{j_type}{p_type}"