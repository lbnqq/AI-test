#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é€æ˜æµæ°´çº¿ - é›†æˆæ™ºèƒ½å›é€€è¯„ä¼°å™¨
è§£å†³APIé™åˆ¶å’Œé»˜è®¤è¯„åˆ†é—®é¢˜ï¼Œç¡®ä¿è¯„ä¼°è´¨é‡
"""

import sys
import os
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from smart_evaluator import SmartEvaluator
from single_report_pipeline.input_parser import InputParser
from single_report_pipeline.context_generator import ContextGenerator
from single_report_pipeline.reverse_scoring_processor import ReverseScoringProcessor

class SmartTransparentPipeline:
    """æ™ºèƒ½é€æ˜æµæ°´çº¿ - é›†æˆæ™ºèƒ½å›é€€è¯„ä¼°å™¨"""

    def __init__(self, use_cloud: bool = True, dispute_threshold: int = 2):
        """
        åˆå§‹åŒ–æ™ºèƒ½é€æ˜æµæ°´çº¿

        Args:
            use_cloud: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨äº‘ç«¯æ¨¡å‹
            dispute_threshold: äº‰è®®æ£€æµ‹é˜ˆå€¼
        """
        self.use_cloud = use_cloud
        self.dispute_threshold = dispute_threshold

        # åˆå§‹åŒ–ç»„ä»¶
        self.input_parser = InputParser()
        self.context_generator = ContextGenerator()
        self.reverse_processor = ReverseScoringProcessor()
        self.smart_evaluator = SmartEvaluator()

        # æ¨¡å‹é…ç½®
        if use_cloud:
            self.primary_models = [
                'deepseek-v3.1:671b-cloud',
                'gpt-oss:120b-cloud',
                'qwen3-vl:235b-cloud'
            ]
            self.dispute_models = [
                'qwen3-vl:235b-cloud',
                'gpt-oss:120b-cloud'
            ]
        else:
            self.primary_models = [
                'qwen3:8b',
                'deepseek-r1:8b',
                'mistral:instruct'
            ]
            self.dispute_models = [
                'qwen3:8b',
                'deepseek-r1:8b'
            ]

        print(f"æ™ºèƒ½é€æ˜æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
        print(f"ä¸»è¦è¯„ä¼°æ¨¡å‹: {self.primary_models}")
        print(f"äº‰è®®è§£å†³æ¨¡å‹: {self.dispute_models}")
        print(f"æ™ºèƒ½å›é€€: âœ… å·²å¯ç”¨")

    def detect_major_dimension_disputes(self, all_scores: List[Dict[str, int]], question: Dict, threshold: int = 2) -> Dict[str, Dict]:
        """
        æ£€æµ‹ä¸»è¦ç»´åº¦çš„äº‰è®®ï¼ˆåªæ£€æŸ¥5ä¸ªæ ¸å¿ƒç»´åº¦ï¼‰

        Args:
            all_scores: æ‰€æœ‰è¯„åˆ†ç»“æœ
            question: é¢˜ç›®ä¿¡æ¯
            threshold: äº‰è®®é˜ˆå€¼

        Returns:
            äº‰è®®ä¿¡æ¯å­—å…¸
        """
        major_traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        disputes = {}

        if len(all_scores) < 2:
            return disputes

        for trait in major_traits:
            scores = [score[trait] for score in all_scores if trait in score]
            if len(scores) >= 2:
                min_score = min(scores)
                max_score = max(scores)
                if max_score - min_score >= threshold:
                    disputes[trait] = {
                        'scores': scores,
                        'range': max_score - min_score,
                        'severity': 'high' if max_score - min_score >= 3 else 'medium'
                    }

        return disputes

    def resolve_disputes_intelligently(self, disputes: Dict, question: Dict, all_models_used: List[str], all_scores_data: List[Dict]) -> List[Dict]:
        """
        æ™ºèƒ½äº‰è®®è§£å†³

        Args:
            disputes: äº‰è®®ä¿¡æ¯
            question: é¢˜ç›®ä¿¡æ¯
            all_models_used: å·²ä½¿ç”¨çš„æ¨¡å‹
            all_scores_data: æ‰€æœ‰è¯„åˆ†æ•°æ®

        Returns:
            è§£å†³åçš„è¯„åˆ†ç»“æœ
        """
        if not disputes:
            return all_scores_data

        question_id = question.get('question_id', 'Unknown')
        print(f"  äº‰è®®è§£å†³ (æ™ºèƒ½å›é€€): {len(disputes)} ä¸ªç»´åº¦å­˜åœ¨åˆ†æ­§")

        # ç”Ÿæˆäº‰è®®è§£å†³ä¸Šä¸‹æ–‡
        context = self.context_generator.generate_dispute_resolution_prompt(question, disputes, all_scores_data)

        # å°è¯•ä½¿ç”¨äº‰è®®è§£å†³æ¨¡å‹
        resolution_scores = []

        for model in self.dispute_models:
            if model in all_models_used:
                continue  # è·³è¿‡å·²ä½¿ç”¨çš„æ¨¡å‹

            try:
                print(f"    ä½¿ç”¨äº‰è®®è§£å†³æ¨¡å‹: {model}")
                scores = self.smart_evaluator.evaluate_with_fallback(
                    context=context,
                    preferred_models=[model],
                    question_id=question_id
                )

                resolution_scores.append({
                    'model': model,
                    'scores': scores,
                    'raw_scores': scores.copy(),
                    'resolution_role': 'dispute_resolver'
                })

                # æ·»åŠ åˆ°æ€»åˆ†æ•°æ®ä¸­
                all_scores_data.append(scores)
                break  # æˆåŠŸä¸€ä¸ªå°±å¤Ÿäº†

            except Exception as e:
                print(f"    âŒ äº‰è®®è§£å†³æ¨¡å‹ {model} å¤±è´¥: {e}")
                continue

        return all_scores_data

    def calculate_final_scores_intelligently(self, all_scores_data: List[Dict], question: Dict) -> Dict[str, Any]:
        """
        æ™ºèƒ½è®¡ç®—æœ€ç»ˆå¾—åˆ†

        Args:
            all_scores_data: æ‰€æœ‰è¯„åˆ†æ•°æ®
            question: é¢˜ç›®ä¿¡æ¯

        Returns:
            æœ€ç»ˆè¯„åˆ†ç»“æœ
        """
        major_traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        # åˆ†ç¦»åˆå§‹è¯„åˆ†å’Œäº‰è®®è§£å†³è¯„åˆ†
        initial_scores = [s for s in all_scores_data if isinstance(s, dict) and 'model' in s]
        resolution_scores = [s for s in all_scores_data if isinstance(s, dict) and not any(m in str(s) for m in ['deepseek-v3.1', 'gpt-oss', 'qwen3-vl'])]

        # è®¡ç®—åŠ æƒå¹³å‡
        final_scores = {}
        trait_details = {}

        for trait in major_traits:
            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆè¯„åˆ†
            valid_scores = []
            model_names = []

            for score_data in all_scores_data:
                if isinstance(score_data, dict):
                    if trait in score_data and isinstance(score_data[trait], (int, float)):
                        valid_scores.append(score_data[trait])
                        model_names.append(score_data.get('model', 'unknown'))

            if not valid_scores:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¯„åˆ†ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€
                context = self.context_generator.generate_evaluation_prompt(question)
                try:
                    fallback_scores = self.smart_evaluator.evaluate_with_fallback(
                        context=context,
                        preferred_models=self.primary_models,
                        question_id=question.get('question_id', 'emergency_fallback')
                    )
                    final_scores[trait] = fallback_scores.get(trait, 3)
                    model_names = ['smart_fallback']
                except Exception as e:
                    print(f"    âŒ æ™ºèƒ½å›é€€å¤±è´¥: {e}")
                    final_scores[trait] = 3  # æœ€åçš„ä¿æŠ¤å€¼
                    model_names = ['emergency_default']
            else:
                # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆäº‰è®®è§£å†³è¯„åˆ†æƒé‡æ›´é«˜ï¼‰
                if len(valid_scores) >= 3:
                    # å»æ‰æœ€é«˜å’Œæœ€ä½åˆ†åå–å¹³å‡
                    valid_scores.sort()
                    middle_scores = valid_scores[1:-1]
                    final_scores[trait] = sum(middle_scores) / len(middle_scores)
                else:
                    final_scores[trait] = sum(valid_scores) / len(valid_scores)

                final_scores[trait] = round(final_scores[trait])

            trait_details[trait] = {
                'final_score': final_scores[trait],
                'valid_scores': valid_scores,
                'models_used': model_names,
                'count': len(valid_scores)
            }

        # è®¡ç®—æ•´ä½“å¯é æ€§
        reliability = self._calculate_reliability_score(trait_details, len(initial_scores), len(resolution_scores))

        result = {
            'final_scores': final_scores,
            'trait_details': trait_details,
            'reliability': reliability,
            'total_evaluations': len(all_scores_data),
            'models_used': list(set([s.get('model', 'unknown') for s in initial_scores])),
            'has_disputes': len(self.detect_major_dimension_disputes([s.get('scores', s) for s in initial_scores if isinstance(s, dict)], question)) > 0
        }

        return result

    def _calculate_reliability_score(self, trait_details: Dict, initial_count: int, resolution_count: int) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        base_reliability = 0.5

        # è¯„åˆ†æ•°é‡å¥–åŠ±
        count_bonus = min(0.3, initial_count * 0.1)

        # ä¸€è‡´æ€§å¥–åŠ±
        consistency_bonus = 0
        for trait, details in trait_details.items():
            scores = details['valid_scores']
            if len(scores) >= 2:
                std_dev = (sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores))**0.5
                if std_dev < 0.5:
                    consistency_bonus += 0.1
                elif std_dev < 1.0:
                    consistency_bonus += 0.05

        consistency_bonus = min(consistency_bonus, 0.3)

        # äº‰è®®è§£å†³å¥–åŠ±
        resolution_bonus = min(0.2, resolution_count * 0.1)

        final_reliability = base_reliability + count_bonus + consistency_bonus + resolution_bonus
        return min(final_reliability, 1.0)

    def calculate_big5_averages(self, question_results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—Big Fiveå¹³å‡åˆ†"""
        if not question_results:
            return {}

        big5_sums = {
            'openness_to_experience': 0,
            'conscientiousness': 0,
            'extraversion': 0,
            'agreeableness': 0,
            'neuroticism': 0
        }

        valid_count = 0
        for result in question_results:
            if result.get('success', True) and 'final_scores' in result:
                scores = result['final_scores']
                for trait in big5_sums:
                    big5_sums[trait] += scores.get(trait, 3)
                valid_count += 1

        if valid_count == 0:
            return {}

        return {trait: sum_score / valid_count for trait, sum_score in big5_sums.items()}

    def infer_mbti_type(self, big5_scores: Dict[str, float]) -> str:
        """ä»Big Fiveå¾—åˆ†æ¨æ–­MBTIç±»å‹"""
        if not big5_scores:
            return "Unknown"

        E = big5_scores.get('extraversion', 3)
        O = big5_scores.get('openness_to_experience', 3)
        C = big5_scores.get('conscientiousness', 3)
        A = big5_scores.get('agreeableness', 3)
        N = big5_scores.get('neuroticism', 3)

        # E/I: å¤–å‘æ€§ vs ç¥ç»è´¨
        e_score = E + (5 - N)
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'

        # S/N: æ„Ÿè§‰ vs ç›´è§‰
        S_preference = 'S' if O <= 3 else 'N'

        # T/F: æ€è€ƒ vs æƒ…æ„Ÿ
        T_preference = 'T' if A <= 3 else 'F'

        # J/P: åˆ¤æ–­ vs çŸ¥è§‰
        J_preference = 'J' if C > 3 else 'P'

        return f"{E_preference}{S_preference}{T_preference}{J_preference}"

    def process_single_question(self, question: Dict, question_idx: int) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªé¢˜ç›®ï¼ˆä½¿ç”¨æ™ºèƒ½è¯„ä¼°å™¨ï¼‰
        """
        question_id = question.get('question_id', 'Unknown')
        question_concept = question['question_data'].get('mapped_ipip_concept', 'Unknown')

        # ç¡®ä¿question_idæ˜¯å­—ç¬¦ä¸²
        if not isinstance(question_id, str):
            question_id = str(question_id)

        is_reversed = self.reverse_processor.is_reverse_item(question_id) or \
                     self.reverse_processor.is_reverse_from_concept(question_concept)

        print(f"å¤„ç†ç¬¬ {question_idx+1:02d} é¢˜ (ID: {question_id})")
        print(f"  é¢˜ç›®æ¦‚å¿µ: {question_concept}")
        print(f"  æ˜¯å¦åå‘: {is_reversed}")
        print(f"  è¢«è¯•å›ç­”: {question['extracted_response'][:100]}...")

        # ç”Ÿæˆè¯„ä¼°ä¸Šä¸‹æ–‡
        context = self.context_generator.generate_evaluation_prompt(question)

        # åˆå§‹è¯„ä¼°ï¼ˆä½¿ç”¨æ™ºèƒ½è¯„ä¼°å™¨ï¼‰
        print(f"  åˆå§‹è¯„ä¼° (ä½¿ç”¨ {len(self.primary_models)} ä¸ªæ¨¡å‹):")
        initial_scores = []

        for i, model in enumerate(self.primary_models):
            try:
                print(f"    â””â”€ ä½¿ç”¨æ™ºèƒ½è¯„ä¼°å™¨è°ƒç”¨æ¨¡å‹ {model} è¯„ä¼°é¢˜ç›® {question_id}...")

                scores = self.smart_evaluator.evaluate_with_fallback(
                    context=context,
                    preferred_models=[model],
                    question_id=question_id
                )

                initial_scores.append({
                    'model': model,
                    'scores': scores,
                    'raw_scores': scores.copy()
                })
                print(f"      âœ… è¯„åˆ†: {scores}")

            except Exception as e:
                print(f"      âŒ æ¨¡å‹ {model} æ™ºèƒ½è¯„ä¼°å¤±è´¥: {e}")
                # æ™ºèƒ½è¯„ä¼°å™¨å†…éƒ¨å·²ç»å¤„ç†äº†å›é€€ï¼Œè¿™é‡Œåªæ˜¯è®°å½•
                continue

        if not initial_scores:
            raise RuntimeError(f"æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•è¯„ä¼°é¢˜ç›® {question_id}")

        # æ£€æµ‹äº‰è®®
        all_initial_scores = [item['scores'] for item in initial_scores]
        disputes = self.detect_major_dimension_disputes(all_initial_scores, question, self.dispute_threshold)

        print(f"  äº‰è®®æ£€æµ‹: {len(disputes)} ä¸ªä¸»è¦ç»´åº¦å­˜åœ¨åˆ†æ­§")
        if disputes:
            for trait, dispute_info in disputes.items():
                print(f"    - {trait}: è¯„åˆ† {dispute_info['scores']}, å·®è· {dispute_info['range']}")

        # äº‰è®®è§£å†³
        current_scores = all_initial_scores.copy()
        all_models_used = [item['model'] for item in initial_scores]

        if disputes:
            print(f"  å¼€å§‹æ™ºèƒ½äº‰è®®è§£å†³...")
            current_scores = self.resolve_disputes_intelligently(
                disputes, question, all_models_used, initial_scores
            )

        # è®¡ç®—æœ€ç»ˆå¾—åˆ†
        final_result = self.calculate_final_scores_intelligently(current_scores, question)

        print(f"  âœ… æœ€ç»ˆå¾—åˆ†: {final_result['final_scores']}")
        print(f"  ğŸ“Š å¯é æ€§: {final_result['reliability']:.3f}")

        return {
            'question_id': question_id,
            'question_concept': question_concept,
            'is_reversed': is_reversed,
            'success': True,
            'final_scores': final_result['final_scores'],
            'trait_details': final_result['trait_details'],
            'reliability': final_result['reliability'],
            'models_used': final_result['models_used'],
            'has_disputes': final_result['has_disputes'],
            'total_evaluations': final_result['total_evaluations']
        }

    def process_single_report(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Šï¼ˆæ™ºèƒ½ç‰ˆæœ¬ï¼‰
        """
        print("=" * 80)
        print("æ™ºèƒ½é€æ˜æµæ°´çº¿ - å¤„ç†æµ‹è¯„æŠ¥å‘Š")
        print("=" * 80)
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        print(f"æ™ºèƒ½å›é€€: âœ… å·²å¯ç”¨")
        print(f"APIé™åˆ¶å¤„ç†: âœ… å·²å¯ç”¨")
        print()

        start_time = time.time()

        try:
            # 1. è§£æè¾“å…¥æ–‡ä»¶
            print("æ­¥éª¤1: è§£æè¾“å…¥æ–‡ä»¶")
            questions = self.input_parser.parse_assessment_json(file_path)
            print(f"  è§£æå®Œæˆ: {len(questions)} é“é¢˜ç›®")
            print()

            # 2. å¤„ç†æ¯é“é¢˜
            print("æ­¥éª¤2: æ™ºèƒ½é€é¢˜å¤„ç†ä¸è¯„ä¼°")
            print("-" * 80)

            all_question_results = []
            successful_questions = 0
            failed_questions = 0

            for i, question in enumerate(questions):
                try:
                    result = self.process_single_question(question, i)
                    all_question_results.append(result)
                    successful_questions += 1
                except Exception as e:
                    print(f"  âŒ é¢˜ç›® {i+1} å¤„ç†å¤±è´¥: {e}")
                    all_question_results.append({
                        'question_id': question.get('question_id', f'q_{i}'),
                        'success': False,
                        'error': str(e)
                    })
                    failed_questions += 1

                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½
                if i < len(questions) - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                    time.sleep(1)

            print()

            # 3. è®¡ç®—æ•´ä½“ç»“æœ
            print("æ­¥éª¤3: è®¡ç®—æ•´ä½“åˆ†æç»“æœ")
            print("-" * 40)

            # è®¡ç®—Big Fiveå¹³å‡åˆ†
            big5_averages = self.calculate_big5_averages(all_question_results)
            overall_reliability = sum(r.get('reliability', 0) for r in all_question_results if r.get('success', False)) / max(1, successful_questions)

            print(f"Big Five å¹³å‡å¾—åˆ†:")
            trait_names = {
                'openness_to_experience': 'å¼€æ”¾æ€§',
                'conscientiousness': 'å°½è´£æ€§',
                'extraversion': 'å¤–å‘æ€§',
                'agreeableness': 'å®œäººæ€§',
                'neuroticism': 'ç¥ç»è´¨'
            }
            for trait, score in big5_averages.items():
                name = trait_names.get(trait, trait)
                print(f"  {name}: {score:.2f}")

            # æ¨æ–­MBTIç±»å‹
            mbti_type = self.infer_mbti_type(big5_averages)
            print(f"æ¨æ–­MBTIç±»å‹: {mbti_type}")

            processing_time = time.time() - start_time

            # ç”Ÿæˆæ™ºèƒ½è¯„ä¼°å™¨çŠ¶æ€æŠ¥å‘Š
            evaluator_report = self.smart_evaluator.get_model_status_report()

            result = {
                'success': True,
                'file_path': file_path,
                'processing_time': round(processing_time, 1),
                'total_questions': len(questions),
                'successful_questions': successful_questions,
                'failed_questions': failed_questions,
                'big5_scores': big5_averages,
                'mbti_type': mbti_type,
                'overall_reliability': round(overall_reliability, 3),
                'question_results': all_question_results,
                'pipeline_info': {
                    'type': 'smart_transparent',
                    'intelligent_fallback': True,
                    'api_limit_handling': True,
                    'no_default_scores': True
                },
                'evaluator_status': evaluator_report
            }

            print()
            print("ğŸ‰ æ™ºèƒ½å¤„ç†å®Œæˆ!")
            print(f"å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
            print(f"æˆåŠŸç‡: {successful_questions}/{len(questions)} ({successful_questions/len(questions)*100:.1f}%)")
            print(f"æ•´ä½“å¯é æ€§: {overall_reliability:.3f}")
            print(f"æ™ºèƒ½å›é€€æ¬¡æ•°: {evaluator_report.get('fallback_count', 0)}")

            return result

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e),
                'processing_time': round(time.time() - start_time, 1)
            }


def test_smart_pipeline():
    """æµ‹è¯•æ™ºèƒ½é€æ˜æµæ°´çº¿"""
    print("ğŸ§  æ™ºèƒ½é€æ˜æµæ°´çº¿æµ‹è¯•")
    print("=" * 50)

    # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
    test_files = [
        "results/readonly-original/asses_deepseek_r1_70b_agent_big_five_50_complete2_a1_e0_t0_0_09271.json",
        "results/readonly-original/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"
    ]

    for test_file in test_files:
        if os.path.exists(test_file):
            print(f"ğŸ“‹ æµ‹è¯•æ–‡ä»¶: {test_file}")
            break
    else:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
        return False

    try:
        # åˆ›å»ºæ™ºèƒ½æµæ°´çº¿
        pipeline = SmartTransparentPipeline(use_cloud=True)

        # å¤„ç†æµ‹è¯•æ–‡ä»¶
        result = pipeline.process_single_report(test_file)

        if result.get('success', False):
            print(f"âœ… æµ‹è¯•æˆåŠŸ!")
            print(f"Big Fiveå¾—åˆ†: {result.get('big5_scores', {})}")
            print(f"MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
            print(f"æ•´ä½“å¯é æ€§: {result.get('overall_reliability', 0):.3f}")
            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    success = test_smart_pipeline()
    sys.exit(0 if success else 1)