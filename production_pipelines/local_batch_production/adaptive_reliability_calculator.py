#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€‚åº”æ€§å…±è¯†ç®—æ³•çš„å¯é æ€§è®¡ç®—å™¨
ä¸“é—¨é’ˆå¯¹æ–°å…±è¯†ç®—æ³•è®¾è®¡çš„å¯é æ€§è¯„ä¼°
"""

import statistics
from typing import List, Dict, Any, Tuple
from collections import Counter


class AdaptiveReliabilityCalculator:
    """
    é€‚åº”æ€§å…±è¯†ç®—æ³•çš„å¯é æ€§è®¡ç®—å™¨

    ä¸“é—¨å¤„ç†æ–°å…±è¯†ç®—æ³•çš„å¯é æ€§è¯„ä¼°ï¼š
    1. åŠ¨æ€è¯„ä¼°å™¨æ•°é‡
    2. å¤šè½®å…±è¯†è¿‡ç¨‹
    3. åˆå¹¶è¯„åˆ†ä¸åŸå§‹è¯„åˆ†çš„å·®å¼‚
    """

    def __init__(self):
        # å¯é æ€§è®¡ç®—å‚æ•°
        self.consensus_quality_weight = 0.4    # å…±è¯†è´¨é‡æƒé‡
        self.evaluator_diversity_weight = 0.3  # è¯„ä¼°å™¨å¤šæ ·æ€§æƒé‡
        self.processing_efficiency_weight = 0.2  # å¤„ç†æ•ˆç‡æƒé‡
        self.final_agreement_weight = 0.1     # æœ€ç»ˆä¸€è‡´æ€§æƒé‡

    def calculate_adaptive_reliability(self, consensus_result: Dict[str, Any],
                                      original_scores: List[int],
                                      processing_history: List[Dict] = None) -> Dict[str, Any]:
        """
        è®¡ç®—é€‚åº”æ€§å…±è¯†ç®—æ³•çš„å¯é æ€§

        Args:
            consensus_result: å…±è¯†ç®—æ³•çš„ç»“æœ
            original_scores: åŸå§‹è¯„åˆ†åˆ—è¡¨
            processing_history: å¤„ç†è¿‡ç¨‹å†å²è®°å½•

        Returns:
            å¯é æ€§è¯„ä¼°ç»“æœ
        """
        final_scores = consensus_result['final_scores']
        consensus_method = consensus_result['consensus_method']
        processing_rounds = consensus_result['processing_rounds']

        # 1. å…±è¯†è´¨é‡è¯„ä¼°
        consensus_quality = self._assess_consensus_quality(
            original_scores, final_scores, consensus_method
        )

        # 2. è¯„ä¼°å™¨å¤šæ ·æ€§è¯„ä¼°
        diversity_score = self._assess_evaluator_diversity(
            original_scores, final_scores, processing_rounds
        )

        # 3. å¤„ç†æ•ˆç‡è¯„ä¼°
        efficiency_score = self._assess_processing_efficiency(
            processing_rounds, consensus_method
        )

        # 4. æœ€ç»ˆä¸€è‡´æ€§è¯„ä¼°
        agreement_score = self._assess_final_agreement(final_scores)

        # 5. ç»¼åˆå¯é æ€§è®¡ç®—
        overall_reliability = (
            self.consensus_quality_weight * consensus_quality +
            self.evaluator_diversity_weight * diversity_score +
            self.processing_efficiency_weight * efficiency_score +
            self.final_agreement_weight * agreement_score
        )

        return {
            'overall_reliability': round(overall_reliability, 3),
            'consensus_quality': round(consensus_quality, 3),
            'evaluator_diversity': round(diversity_score, 3),
            'processing_efficiency': round(efficiency_score, 3),
            'final_agreement': round(agreement_score, 3),
            'detailed_analysis': {
                'original_scores': original_scores,
                'final_scores': final_scores,
                'processing_rounds': processing_rounds,
                'consensus_method': consensus_method,
                'score_transformation': self._analyze_score_transformation(
                    original_scores, final_scores
                )
            }
        }

    def _assess_consensus_quality(self, original_scores: List[int],
                                final_scores: List[int],
                                consensus_method: str) -> float:
        """è¯„ä¼°å…±è¯†è´¨é‡"""

        # åŸºäºå…±è¯†æ–¹æ³•çš„åŸºç¡€åˆ†æ•°
        method_scores = {
            'perfect_consensus': 1.0,      # å®Œå…¨å…±è¯†ï¼Œæœ€é«˜è´¨é‡
            'minor_consensus': 0.8,        # è½»å¾®åˆ†æ­§ï¼Œé«˜è´¨é‡
            'median_consensus': 0.7,       # ä¸­ä½æ•°å…±è¯†ï¼Œè¾ƒé«˜è´¨é‡
            'average_consensus': 0.6,      # å¹³å‡æ•°å…±è¯†ï¼Œä¸­ç­‰è´¨é‡
            'extended_consensus': 0.5,     # æ‰©å±•å…±è¯†ï¼Œä¸­ç­‰è´¨é‡
            'max_divergence_consensus': 0.4 # æœ€å¤§åˆ†æ­§å…±è¯†ï¼Œéœ€è¦æ”¹è¿›
        }

        base_score = method_scores.get(consensus_method, 0.3)

        # è¯„åˆ†æ”¹å–„ç¨‹åº¦è°ƒæ•´
        original_range = max(original_scores) - min(original_scores)
        final_range = max(final_scores) - min(final_scores)

        if original_range > 0:
            improvement = (original_range - final_range) / original_range
            quality_adjustment = min(improvement * 0.2, 0.2)  # æœ€å¤šæå‡0.2
        else:
            quality_adjustment = 0.0

        return min(base_score + quality_adjustment, 1.0)

    def _assess_evaluator_diversity(self, original_scores: List[int],
                                   final_scores: List[int],
                                   processing_rounds: int) -> float:
        """è¯„ä¼°è¯„ä¼°å™¨å¤šæ ·æ€§"""

        # 1. åŸå§‹è¯„ä¼°å™¨çš„å¤šæ ·æ€§
        original_diversity = len(set(original_scores)) / len(original_scores)

        # 2. æœ€ç»ˆè¯„åˆ†çš„å¤šæ ·æ€§
        final_diversity = len(set(final_scores)) / len(final_scores)

        # 3. å¤„ç†è½®æ•°çš„åˆç†æ€§ï¼ˆè½®æ•°è¶Šå¤šï¼Œè¯´æ˜åˆ†æ­§è¶Šå¤§ï¼Œä½†æœ€ç»ˆè§£å†³äº†ï¼‰
        round_efficiency = max(0.0, 1.0 - (processing_rounds - 1) * 0.2)

        # ç»¼åˆå¤šæ ·æ€§åˆ†æ•°
        diversity_score = (
            0.4 * original_diversity +
            0.4 * final_diversity +
            0.2 * round_efficiency
        )

        return diversity_score

    def _assess_processing_efficiency(self, processing_rounds: int,
                                    consensus_method: str) -> float:
        """è¯„ä¼°å¤„ç†æ•ˆç‡"""

        # åŸºç¡€æ•ˆç‡åˆ†æ•°ï¼ˆè½®æ•°è¶Šå°‘æ•ˆç‡è¶Šé«˜ï¼‰
        if processing_rounds == 1:
            round_efficiency = 1.0
        elif processing_rounds == 2:
            round_efficiency = 0.8
        else:
            round_efficiency = max(0.4, 1.0 - (processing_rounds - 2) * 0.2)

        # æ ¹æ®å…±è¯†æ–¹æ³•è°ƒæ•´
        method_efficiency = {
            'perfect_consensus': 1.0,      # ä¸€æ¬¡æ€§è¾¾æˆï¼Œæœ€é«˜æ•ˆ
            'minor_consensus': 0.9,        # è½»å¾®å¤„ç†ï¼Œé«˜æ•ˆ
            'median_consensus': 0.8,       # ä¸­ä½æ•°å¤„ç†ï¼Œè¾ƒé«˜æ•ˆ
            'average_consensus': 0.7,      # å¹³å‡æ•°å¤„ç†ï¼Œä¸­ç­‰æ•ˆç‡
            'extended_consensus': 0.6,     # éœ€è¦æ‰©å±•ï¼Œæ•ˆç‡è¾ƒä½
            'max_divergence_consensus': 0.5 # æœ€å¤§åˆ†æ­§å¤„ç†ï¼Œæ•ˆç‡æœ€ä½
        }

        method_factor = method_efficiency.get(consensus_method, 0.5)

        return (round_efficiency + method_factor) / 2

    def _assess_final_agreement(self, final_scores: List[int]) -> float:
        """è¯„ä¼°æœ€ç»ˆä¸€è‡´æ€§"""

        if len(final_scores) < 2:
            return 1.0

        # 1. æ ‡å‡†å·®ä¸€è‡´æ€§
        std_dev = statistics.stdev(final_scores)
        max_possible_std = 2.0  # 1-5è¯„åˆ†åˆ¶çš„æœ€å¤§æ ‡å‡†å·®
        consistency_score = max(0.0, 1.0 - (std_dev / max_possible_std))

        # 2. ä¼—æ•°æ¯”ä¾‹
        score_counts = Counter(final_scores)
        max_count = max(score_counts.values())
        mode_ratio = max_count / len(final_scores)

        # 3. è¯„åˆ†èŒƒå›´
        score_range = max(final_scores) - min(final_scores)
        range_score = max(0.0, 1.0 - (score_range / 4.0))  # æœ€å¤§èŒƒå›´æ˜¯4

        # ç»¼åˆä¸€è‡´æ€§
        agreement = 0.4 * consistency_score + 0.4 * mode_ratio + 0.2 * range_score

        return agreement

    def _analyze_score_transformation(self, original_scores: List[int],
                                    final_scores: List[int]) -> Dict[str, Any]:
        """åˆ†æè¯„åˆ†è½¬æ¢è¿‡ç¨‹"""

        original_mean = statistics.mean(original_scores)
        final_mean = statistics.mean(final_scores)

        original_median = statistics.median(original_scores)
        final_median = statistics.median(final_scores)

        return {
            'original_mean': round(original_mean, 2),
            'final_mean': round(final_mean, 2),
            'mean_change': round(final_mean - original_mean, 2),
            'original_median': original_median,
            'final_median': final_median,
            'median_change': final_median - original_median,
            'score_count_change': len(final_scores) - len(original_scores)
        }


def demonstrate_adaptive_reliability():
    """æ¼”ç¤ºé€‚åº”æ€§å¯é æ€§è®¡ç®—"""
    print("ğŸ”§ é€‚åº”æ€§å…±è¯†ç®—æ³•å¯é æ€§è®¡ç®—æ¼”ç¤º")
    print("=" * 60)

    calculator = AdaptiveReliabilityCalculator()

    # æ¨¡æ‹Ÿå…±è¯†ç»“æœ
    test_cases = [
        {
            'name': 'å®Œå…¨å…±è¯†åœºæ™¯',
            'consensus_result': {
                'consensus_score': 3.0,
                'final_scores': [3, 3, 3],
                'consensus_method': 'perfect_consensus',
                'processing_rounds': 1
            },
            'original_scores': [3, 3, 3]
        },
        {
            'name': 'è½»å¾®åˆ†æ­§åœºæ™¯',
            'consensus_result': {
                'consensus_score': 3.67,
                'final_scores': [3, 3, 5],
                'consensus_method': 'minor_consensus',
                'processing_rounds': 1
            },
            'original_scores': [3, 3, 5]
        },
        {
            'name': 'ä¸¥é‡åˆ†æ­§å¤„ç†å',
            'consensus_result': {
                'consensus_score': 2.33,
                'final_scores': [1, 3, 3],
                'consensus_method': 'extended_consensus',
                'processing_rounds': 2
            },
            'original_scores': [1, 1, 5]
        },
        {
            'name': 'æœ€å¤§åˆ†æ­§å¤„ç†å',
            'consensus_result': {
                'consensus_score': 3.0,
                'final_scores': [3, 3, 3],
                'consensus_method': 'max_divergence_consensus',
                'processing_rounds': 2
            },
            'original_scores': [1, 3, 5]
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯ {i}: {test_case['name']}")
        print(f"åŸå§‹è¯„åˆ†: {test_case['original_scores']}")
        print(f"æœ€ç»ˆè¯„åˆ†: {test_case['consensus_result']['final_scores']}")
        print(f"å…±è¯†æ–¹æ³•: {test_case['consensus_result']['consensus_method']}")
        print("-" * 50)

        reliability = calculator.calculate_adaptive_reliability(
            test_case['consensus_result'],
            test_case['original_scores']
        )

        print(f"ğŸ¯ å¯é æ€§è¯„ä¼°ç»“æœ:")
        print(f"  æ€»ä½“å¯é æ€§: {reliability['overall_reliability']:.3f}")
        print(f"  å…±è¯†è´¨é‡: {reliability['consensus_quality']:.3f}")
        print(f"  è¯„ä¼°å™¨å¤šæ ·æ€§: {reliability['evaluator_diversity']:.3f}")
        print(f"  å¤„ç†æ•ˆç‡: {reliability['processing_efficiency']:.3f}")
        print(f"  æœ€ç»ˆä¸€è‡´æ€§: {reliability['final_agreement']:.3f}")


if __name__ == "__main__":
    demonstrate_adaptive_reliability()