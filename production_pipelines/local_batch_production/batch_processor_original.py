#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†å™¨ - é«˜æ•ˆç¨³å®šçš„æ‰¹é‡æµ‹è¯„æŠ¥å‘Šå¤„ç†
æ”¯æŒ50é¢˜æ–‡ä»¶ã€æ–­ç‚¹ç»­è·‘ã€è¶…æ—¶ä¿æŠ¤ã€å¢å¼ºç®—æ³•
"""

import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime
import time
import argparse
import logging
import pickle
from typing import List, Dict, Any, Tuple

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ - é«˜æ•ˆç¨³å®šçš„æ‰¹é‡æµ‹è¯„æŠ¥å‘Šå¤„ç†"""

    def __init__(self, input_dir: str, output_dir: str,
                 max_evaluators: int = 3,
                 use_enhanced: bool = False):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            max_evaluators: æœ€å¤§è¯„ä¼°å™¨æ•°é‡
            use_enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºæµæ°´çº¿
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.max_evaluators = max_evaluators
        self.use_enhanced = use_enhanced

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        self.checkpoint_file = self.output_dir / "batch_checkpoint.pkl"
        self.results_file = self.output_dir / "batch_results.json"
        self.summary_file = self.output_dir / "batch_summary.md"
        self.log_file = self.output_dir / "batch_processing.log"

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–æµæ°´çº¿
        if use_enhanced:
            from single_report_pipeline.enhanced_transparent_pipeline import EnhancedTransparentPipeline
            self.pipeline = EnhancedTransparentPipeline(use_cloud=False)
            self.logger.info("âœ… ä½¿ç”¨å¢å¼ºæµæ°´çº¿ï¼ˆæ–°ç®—æ³•ï¼‰")
        else:
            self.pipeline = TransparentPipeline(use_cloud=False)
            self.logger.info("âœ… ä½¿ç”¨åŸæµæ°´çº¿")

        # åˆå§‹åŒ–çŠ¶æ€
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
        self.total_files = 0
        self.current_file_index = 0

        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.timeout_per_question = 300  # 5åˆ†é’Ÿè¶…æ—¶
        self.retry_count = 2  # é‡è¯•æ¬¡æ•°

        # é—®é¢˜æŠ¥å‘Šç­›é€‰å‚æ•°
        self.problem_reports_dir = self.output_dir / "problem_reports"
        self.problem_reports_dir.mkdir(exist_ok=True)
        self.problem_reports_count = 0

        # åˆå§‹åŒ–é—®é¢˜æŠ¥å‘Šè¯†åˆ«æ¨¡å¼
        self._init_problem_patterns()

    def _init_problem_patterns(self):
        """åˆå§‹åŒ–é—®é¢˜æŠ¥å‘Šè¯†åˆ«æ¨¡å¼"""
        self.problem_patterns = [
            # è‹±æ–‡é—®é¢˜æ¨¡å¼
            r'please provide me with the prompt',
            r'please provide me with a prompt',
            r'please provide me with a prompt so I can assist you',
            r'please provide me with a prompt so I can help you',
            r'can you give me the prompt',
            r'what is the prompt',
            r'please provide the prompt',
            r'can i see the prompt',
            r'i need the prompt',
            r'what should i do',
            r'tell me what to do',
            r'please give me instructions',
            r'what task should i perform',
            r'i\'m ready for any task',
            r'ready for any task',
            r'please provide the question',
            r'what is the question',
            r'can you provide the question',
            r'so I can assist you',
            r'so I can help you',
            r'I\'m ready for any task, question or creative',
            r'I\'m ready for any task, question or creative writing',

            # ä¸­æ–‡é—®é¢˜æ¨¡å¼
            r'è¯·æä¾›æç¤ºè¯',
            r'è¯·æä¾›é¢˜ç›®',
            r'è¯·ç»™å‡ºé—®é¢˜',
            r'è¯·ç»™å‡ºæç¤ºè¯',
            r'æˆ‘æ²¡æœ‰çœ‹åˆ°é¢˜ç›®',
            r'æ²¡æœ‰çœ‹åˆ°é—®é¢˜',
            r'è¯·å‘Šè¯‰æˆ‘é—®é¢˜',
            r'å¯ä»¥å‘Šè¯‰æˆ‘é—®é¢˜æ˜¯ä»€ä¹ˆå—',
            r'èƒ½å‘Šè¯‰æˆ‘é—®é¢˜æ˜¯ä»€ä¹ˆå—',
            r'éœ€è¦æç¤ºè¯',
            r'è¯·æä¾›å…·ä½“é—®é¢˜',
            r'è¯·è¯´æ˜ä»»åŠ¡',
            r'æˆ‘åº”è¯¥åšä»€ä¹ˆ',
            r'è¯·æä¾›å…·ä½“è¦æ±‚',

            # æ¨¡ç³Šé—®é¢˜æ¨¡å¼
            r'no prompt',
            r'no question',
            r'no instructions',
            r'missing prompt',
            r'missing question',
            r'æ²¡æœ‰æç¤ºè¯',
            r'æ²¡æœ‰é—®é¢˜',
            r'æ²¡æœ‰é¢˜ç›®',
            r'ç¼ºå°‘æç¤ºè¯',
            r'ç¼ºå°‘é—®é¢˜',

            # ç³»ç»Ÿå›åº”æ¨¡å¼
            r'i am an ai',
            r'i\'m an ai',
            r'i am a language model',
            r'i\'m a language model',
            r'i cannot provide',
            r'i cannot answer',
            r'as an ai',
            r'as an assistant',

            # è§’è‰²æ‰®æ¼”å’Œä¸ç›¸å…³å›ç­”æ¨¡å¼
            r'ä»€ä¹ˆé—®é¢˜',
            r'è¯·æè¿°ä¸€ä¸‹',
            r'è¯·è¯´æ˜',
            r'èƒ½è¯¦ç»†è¯´æ˜',
            r'å¯ä»¥è¯¦ç»†æè¿°',
            r'å…·ä½“æ˜¯ä»€ä¹ˆ',
            r'è¯·æä¾›å…·ä½“',
            r'è¯·æ›´è¯¦ç»†',
            r'éœ€è¦æ›´å¤šä¿¡æ¯',
            r'èƒ½è¯¦ç»†è§£é‡Š',
            r'è¯·è¯¦ç»†è§£é‡Š',

            # æ‹¬å·åŠ¨ä½œæè¿°æ¨¡å¼ (è§’è‰²æ‰®æ¼”)
            r'ï¼ˆ.*?ï¼‰',
            r'\(.*?\)',
            r'\ã€.*?\ã€‘',
            r'\[.*?\]',

            # é€šç”¨æ— æ„ä¹‰å›åº”
            r'å¥½çš„ï¼Œæ˜ç™½äº†',
            r'äº†è§£äº†',
            r'æ”¶åˆ°',
            r'çŸ¥é“äº†',
            r'å—¯ï¼Œå¥½çš„',
            r'å¥½çš„ï¼Œè¯·è¯´',
            r'è¯·ç»§ç»­',
            r'è¯·è®²',
        ]

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE | re.DOTALL)
                                 for pattern in self.problem_patterns]

    def check_problem_response(self, response_text: str) -> bool:
        """æ£€æŸ¥å•ä¸ªå›ç­”æ˜¯å¦ä¸ºé—®é¢˜å›ç­”"""
        if not response_text or not isinstance(response_text, str):
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•é—®é¢˜æ¨¡å¼
        for pattern in self.compiled_patterns:
            if pattern.search(response_text):
                return True

        # æ£€æŸ¥è¿‡çŸ­å›ç­”
        if len(response_text.strip()) < 10:
            return True

        # æ£€æŸ¥æ˜¯å¦åªæ˜¯æ ‡ç‚¹ç¬¦å·
        if re.match(r'^[?.!]+$', response_text.strip()):
            return True

        return False

    def is_problem_report(self, file_path: str) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé—®é¢˜æµ‹è¯„æŠ¥å‘Š

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            (is_problem, reason): æ˜¯å¦ä¸ºé—®é¢˜æŠ¥å‘Š, åŸå› 
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                report_data = json.load(f)

            assessment_results = report_data.get('assessment_results', [])
            if not assessment_results:
                return True, "empty_assessment_results"

            total_questions = len(assessment_results)
            problem_responses = 0

            for result in assessment_results:
                extracted_response = result.get('extracted_response', '')
                if self.check_problem_response(extracted_response):
                    problem_responses += 1

            # å¦‚æœè¶…è¿‡30%çš„å›ç­”éƒ½æ˜¯é—®é¢˜å›ç­”ï¼Œåˆ™æ ‡è®°ä¸ºé—®é¢˜æŠ¥å‘Š
            problem_ratio = problem_responses / total_questions
            if problem_ratio >= 0.3:
                reason = f"é—®é¢˜å›ç­”æ¯”ä¾‹: {problem_responses}/{total_questions} ({problem_ratio:.1%})"
                return True, reason

            return False, ""

        except Exception as e:
            return True, f"æ£€æŸ¥å¤±è´¥: {str(e)}"

    def handle_problem_report(self, file_path: Path, reason: str):
        """å¤„ç†é—®é¢˜æµ‹è¯„æŠ¥å‘Š"""
        try:
            # å¤åˆ¶æ–‡ä»¶åˆ°é—®é¢˜æŠ¥å‘Šç›®å½•
            problem_file = self.problem_reports_dir / file_path.name
            import shutil
            shutil.copy2(file_path, problem_file)

            # ä¿å­˜é—®é¢˜è¯¦æƒ…
            detail_file = self.problem_reports_dir / f"{file_path.stem}_problem_details.txt"
            with open(detail_file, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡ä»¶: {file_path.name}\n")
                f.write(f"é—®é¢˜åŸå› : {reason}\n")
                f.write(f"æ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

            self.problem_reports_count += 1
            self.logger.warning(f"ğŸš© é—®é¢˜æŠ¥å‘Šå·²æ ‡è®°: {file_path.name} - {reason}")

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†é—®é¢˜æŠ¥å‘Šå¤±è´¥ {file_path.name}: {e}")

    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)

                self.processed_files = set(checkpoint_data.get('processed_files', []))
                self.results = checkpoint_data.get('results', [])
                self.start_time = checkpoint_data.get('start_time', datetime.now())
                self.total_files = checkpoint_data.get('total_files', 0)
                self.current_file_index = checkpoint_data.get('current_file_index', 0)

                self.logger.info(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: å¤„ç†äº† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
                return True
            except Exception as e:
                self.logger.warning(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return False
        else:
            self.logger.info("â„¹ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return False

    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'start_time': self.start_time,
            'total_files': self.total_files,
            'current_file_index': self.current_file_index
        }

        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            self.logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(self.processed_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def process_single_file_optimized(self, file_path: str) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„å•æ–‡ä»¶å¤„ç†

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†ç»“æœ
        """
        self.logger.info(f"ğŸ” ä¼˜åŒ–å¤„ç†: {Path(file_path).name}")

        start_time = time.time()

        try:
            # è§£ææ–‡ä»¶
            from single_report_pipeline.input_parser import InputParser
            parser = InputParser()
            questions = parser.parse_assessment_json(file_path)

            total_questions = len(questions)
            self.logger.info(f"   é¢˜ç›®æ€»æ•°: {total_questions} (å…¨éƒ¨å¤„ç†)")

            # å¤„ç†æ‰€æœ‰é¢˜ç›®
            all_question_results = []

            for i, question in enumerate(questions):
                question_id = question.get('question_id', f'Q{i+1}')
                self.logger.info(f"   å¤„ç†é¢˜ç›® {i+1}/{total_questions}: {question_id}")

                try:
                    # Windowså…¼å®¹çš„è¶…æ—¶å¤„ç†
                    import threading

                    result_container = {}
                    exception_container = {}

                    def worker():
                        try:
                            result = self.pipeline.process_single_question(question, i)
                            result_container['result'] = result
                        except Exception as e:
                            exception_container['exception'] = e

                    # å¯åŠ¨å·¥ä½œçº¿ç¨‹
                    thread = threading.Thread(target=worker)
                    thread.daemon = True
                    thread.start()

                    # ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
                    thread.join(timeout=self.timeout_per_question)

                    if thread.is_alive():
                        # è¶…æ—¶å¤„ç†
                        self.logger.warning(f"      âš ï¸ é¢˜ç›®å¤„ç†è¶…æ—¶: {question_id}")
                        default_result = {
                            'question_id': question_id,
                            'final_adjusted_scores': {
                                'openness_to_experience': 3,
                                'conscientiousness': 3,
                                'extraversion': 3,
                                'agreeableness': 3,
                                'neuroticism': 3
                            },
                            'confidence_metrics': {
                                'overall_reliability': 0.5,
                                'trait_reliabilities': {
                                    'openness_to_experience': 0.5,
                                    'conscientiousness': 0.5,
                                    'extraversion': 0.5,
                                    'agreeableness': 0.5,
                                    'neuroticism': 0.5
                                }
                            },
                            'timeout': True
                        }
                        all_question_results.append(default_result)
                    elif 'exception' in exception_container:
                        # å¼‚å¸¸å¤„ç†
                        self.logger.error(f"      âŒ å¤„ç†å¤±è´¥: {exception_container['exception']}")
                        continue
                    else:
                        # æˆåŠŸå¤„ç†
                        result = result_container['result']
                        all_question_results.append(result)

                        # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
                        reliability = result['confidence_metrics']['overall_reliability']
                        models_used = len(result['models_used'])
                        self.logger.info(f"      âœ… å®Œæˆ - å¯é æ€§: {reliability:.3f}, æ¨¡å‹æ•°: {models_used}")

                except Exception as e:
                    self.logger.error(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                    continue

            # å¿«é€Ÿè®¡ç®—Big5å¾—åˆ†ï¼ˆé¿å…å®Œæ•´æµæ°´çº¿è®¡ç®—ï¼‰
            big5_scores = self.calculate_big5_scores_fast(all_question_results)
            mbti_type = self.calculate_mbti_fast(big5_scores)

            processing_time = time.time() - start_time

            result = {
                'file_path': file_path,
                'total_questions': total_questions,
                'processed_questions': len(all_question_results),
                'big5_scores': big5_scores,
                'mbti_type': mbti_type,
                'question_results': all_question_results,
                'processing_time': processing_time,
                'algorithm_info': {
                    'max_evaluators': self.max_evaluators,
                    'use_enhanced': self.use_enhanced
                },
                'summary': {
                    'openness': big5_scores.get('openness_to_experience', 3.0),
                    'conscientiousness': big5_scores.get('conscientiousness', 3.0),
                    'extraversion': big5_scores.get('extraversion', 3.0),
                    'agreeableness': big5_scores.get('agreeableness', 3.0),
                    'neuroticism': big5_scores.get('neuroticism', 3.0),
                    'processing_time': round(processing_time, 2)
                }
            }

            self.logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {Path(file_path).name} ({processing_time:.2f}ç§’)")
            return result

        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path} - {e}")
            return None

    def calculate_big5_scores_fast(self, question_results: List[Dict]) -> Dict[str, float]:
        """å¿«é€Ÿè®¡ç®—Big5å¾—åˆ†"""
        if not question_results:
            return {
                'openness_to_experience': 3.0,
                'conscientiousness': 3.0,
                'extraversion': 3.0,
                'agreeableness': 3.0,
                'neuroticism': 3.0
            }

        # ç®€åŒ–è®¡ç®—ï¼šç›´æ¥å–æ‰€æœ‰é¢˜ç›®çš„å¹³å‡å€¼
        dimensions = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        big5_scores = {}

        for dimension in dimensions:
            scores = []
            for result in question_results:
                if 'final_adjusted_scores' in result:
                    score = result['final_adjusted_scores'].get(dimension, 3)
                    if score in [1, 3, 5]:
                        scores.append(score)

            if scores:
                big5_scores[dimension] = round(sum(scores) / len(scores), 2)
            else:
                big5_scores[dimension] = 3.0

        return big5_scores

    def calculate_mbti_fast(self, big5_scores: Dict[str, float]) -> str:
        """å¿«é€Ÿè®¡ç®—MBTIç±»å‹"""
        O = big5_scores.get('openness_to_experience', 3)
        C = big5_scores.get('conscientiousness', 3)
        E = big5_scores.get('extraversion', 3)
        A = big5_scores.get('agreeableness', 3)
        N = big5_scores.get('neuroticism', 3)

        # E/I: å¤–å‘æ€§ vs ç¥ç»è´¨
        e_score = E + (5 - N)
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'

        # S/N: æ„Ÿè§‰ vs ç›´è§‰ (åŸºäºå¼€æ”¾æ€§)
        S_preference = 'S' if O <= 3 else 'N'

        # T/F: æ€è€ƒ vs æƒ…æ„Ÿ (åŸºäºå®œäººæ€§)
        T_preference = 'T' if A <= 3 else 'F'

        # J/P: åˆ¤æ–­ vs çŸ¥è§‰ (åŸºäºå°½è´£æ€§)
        J_preference = 'J' if C > 3 else 'P'

        return f"{E_preference}{S_preference}{T_preference}{J_preference}"

    def run(self):
        """è¿è¡Œæ‰¹é‡å¤„ç†"""
        self.logger.info("ğŸš€ æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨")
        self.logger.info("=" * 80)
        self.logger.info(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"å¤„ç†å‚æ•°: å…¨éƒ¨é¢˜ç›®å¤„ç†, æœ€å¤§{self.max_evaluators}ä¸ªè¯„ä¼°å™¨")
        self.logger.info(f"ç®—æ³•é€‰æ‹©: {'å¢å¼ºç®—æ³•' if self.use_enhanced else 'åŸç®—æ³•'}")

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_loaded = self.load_checkpoint()

        # æŸ¥æ‰¾æ–‡ä»¶
        files = list(self.input_dir.glob("*.json"))
        files = [f for f in files if f.is_file() and not f.name.startswith('.')]

        if not files:
            self.logger.error("âŒ æœªæ‰¾åˆ°æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return

        self.total_files = len(files)
        self.logger.info(f"ğŸ“‚ æ‰¾åˆ° {self.total_files} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        self.logger.info(f"   å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        self.logger.info(f"   å‰©ä½™: {self.total_files - len(self.processed_files)} ä¸ª")

        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        remaining_files = [f for f in files if str(f) not in self.processed_files]
        remaining_files.sort(key=lambda x: x.name)

        self.logger.info("")
        self.logger.info(f"â–¶ï¸  ä»ç¬¬ {self.current_file_index + 1} ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        self.logger.info("")

        # å¤„ç†æ–‡ä»¶
        for i, file_path in enumerate(remaining_files):
            self.current_file_index = i + len(self.processed_files)

            self.logger.info(f"ğŸ“ è¿›åº¦: {self.current_file_index}/{self.total_files} æ–‡ä»¶")

            # æ£€æŸ¥æ˜¯å¦ä¸ºé—®é¢˜æŠ¥å‘Š
            is_problem, reason = self.is_problem_report(str(file_path))
            if is_problem:
                self.handle_problem_report(file_path, reason)
                self.processed_files.add(str(file_path))  # æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…é‡å¤
                continue  # è·³è¿‡æ­£å¸¸å¤„ç†æµç¨‹

            # æ­£å¸¸å¤„ç†æ–‡ä»¶
            result = self.process_single_file_optimized(str(file_path))

            if result:
                self.results.append(result)
                self.processed_files.add(str(file_path))

                # æ¯å¤„ç†ä¸€ä¸ªæ–‡ä»¶å°±ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint()

                # ä¿å­˜ä¸­é—´ç»“æœ
                self.save_intermediate_results()
            else:
                self.logger.error(f"âŒ è·³è¿‡æ–‡ä»¶: {file_path}")
                continue

        # æœ€ç»ˆæ±‡æ€»
        self.generate_final_summary()
        self.logger.info("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")

        return self.results

    def save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

    def generate_final_summary(self):
        """ç”Ÿæˆæœ€ç»ˆæ±‡æ€»"""
        if not self.results:
            self.logger.warning("âš ï¸ æ²¡æœ‰å¤„ç†ç»“æœï¼Œè·³è¿‡æ±‡æ€»ç”Ÿæˆ")
            return

        end_time = datetime.now()
        total_time = (end_time - self.start_time).total_seconds()

        # ç”ŸæˆMarkdownæ‘˜è¦
        summary_content = f"""# æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦

## ğŸ“Š å¤„ç†ç»Ÿè®¡
- **æ€»æ–‡ä»¶æ•°**: {self.total_files}
- **æœ‰æ•ˆå¤„ç†**: {len(self.results)}
- **é—®é¢˜æŠ¥å‘Š**: {self.problem_reports_count}
- **å¤„ç†å¤±è´¥**: {self.total_files - len(self.processed_files)}
- **å¤„ç†æˆåŠŸç‡**: {len(self.results)/self.total_files:.1%}
- **æ€»å¤„ç†æ—¶é—´**: {total_time:.1f}ç§’

## ğŸš© é—®é¢˜æŠ¥å‘Šç­›é€‰
- **ç­›é€‰æ ‡å‡†**: 30%ä»¥ä¸Šå›ç­”ä¸ºé—®é¢˜å›ç­”ï¼ˆå¦‚"è¯·æä¾›æç¤ºè¯"ç­‰ï¼‰
- **é—®é¢˜æŠ¥å‘Šæ•°é‡**: {self.problem_reports_count} ä¸ª
- **é—®é¢˜æŠ¥å‘Šæ¯”ä¾‹**: {self.problem_reports_count/self.total_files:.1%}
- **é—®é¢˜æŠ¥å‘Šç›®å½•**: `{self.problem_reports_dir}`

## âš™ï¸ å¤„ç†å‚æ•°
- **å¤„ç†é¢˜ç›®æ•°**: å…¨éƒ¨50é¢˜
- **æœ€å¤§è¯„ä¼°å™¨æ•°é‡**: {self.max_evaluators}
- **ä½¿ç”¨å¢å¼ºç®—æ³•**: {'æ˜¯' if self.use_enhanced else 'å¦'}
- **è¶…æ—¶è®¾ç½®**: {self.timeout_per_question}ç§’/é¢˜ç›®

## ğŸ“‹ æ–‡ä»¶å¤„ç†ç»“æœ
"""

        for result in self.results:
            file_name = Path(result['file_path']).name
            big5 = result.get('big5_scores', {})
            mbti = result.get('mbti_type', 'Unknown')
            time_used = result.get('processing_time', 0)
            questions = result.get('processed_questions', 0)

            summary_content += f"""
### {file_name}
- **é¢˜ç›®æ•°**: {questions}/{result.get('total_questions', 0)}
- **Big5å¾—åˆ†**: {big5}
- **MBTIç±»å‹**: {mbti}
- **å¤„ç†æ—¶é—´**: {time_used:.2f}ç§’
"""

        summary_content += f"""

## ğŸ¯ æ•´ä½“ç»Ÿè®¡
"""

        # è®¡ç®—å¹³å‡Big5å¾—åˆ†
        if self.results:
            avg_scores = {
                'openness_to_experience': 0,
                'conscientiousness': 0,
                'extraversion': 0,
                'agreeableness': 0,
                'neuroticism': 0
            }

            for result in self.results:
                big5 = result.get('big5_scores', {})
                for dim in avg_scores:
                    avg_scores[dim] += big5.get(dim, 0)

            for dim in avg_scores:
                avg_scores[dim] = round(avg_scores[dim] / len(self.results), 2)

            summary_content += f"""
- **å¹³å‡Big5å¾—åˆ†**: {avg_scores}
"""

        summary_content += f"""

## ğŸ’¡ å¤„ç†è¯´æ˜
- **å®Œæ•´50é¢˜å¤„ç†**: ç”Ÿäº§ç‰ˆæœ¬å¤„ç†å®Œæ•´çš„50é¢˜ç›®æµ‹è¯„æŠ¥å‘Š
- **é—®é¢˜æŠ¥å‘Šç­›é€‰**: è‡ªåŠ¨è¯†åˆ«å¹¶ç­›é€‰è¢«è¯•æœªæ­£ç¡®çœ‹åˆ°é¢˜ç›®æç¤ºçš„æŠ¥å‘Š
- **æ–­ç‚¹ç»­è·‘**: æ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­å¤„ç†ï¼Œé¿å…é‡å¤å·¥ä½œ
- **è¶…æ—¶ä¿æŠ¤**: æ¯é¢˜5åˆ†é’Ÿè¶…æ—¶ï¼Œé˜²æ­¢å› ä¸ªåˆ«é—®é¢˜å¡ä½æ•´ä½“è¿›åº¦

## ğŸš© é—®é¢˜æŠ¥å‘Šè¯´æ˜
é—®é¢˜æŠ¥å‘Šæ˜¯æŒ‡è¢«è¯•å¯èƒ½æ²¡æœ‰æ­£ç¡®çœ‹åˆ°é¢˜ç›®æç¤ºè¯çš„æµ‹è¯„æŠ¥å‘Šï¼Œè¡¨ç°ä¸ºï¼š
- å›ç­”"è¯·æä¾›æç¤ºè¯"ã€"è¯·ç»™å‡ºé—®é¢˜"ç­‰
- å›ç­”è¿‡çŸ­æˆ–ä»…ä¸ºæ ‡ç‚¹ç¬¦å·
- é—®é¢˜å›ç­”æ¯”ä¾‹è¶…è¿‡30%çš„æŠ¥å‘Š

è¿™äº›æŠ¥å‘Šå·²è¢«å•ç‹¬åˆ†ç±»ä¿å­˜ï¼Œä¸å½±å“æ­£å¸¸æŠ¥å‘Šçš„å¤„ç†ç»“æœã€‚
"""

        try:
            with open(self.summary_file, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            self.logger.info(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {self.summary_file}")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†å™¨')
    parser.add_argument('--input-dir', required=True, help='è¾“å…¥ç›®å½•')
    parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-evaluators', type=int, default=3, help='æœ€å¤§è¯„ä¼°å™¨æ•°é‡')
    parser.add_argument('--enhanced', action='store_true', help='ä½¿ç”¨å¢å¼ºç®—æ³•')
    parser.add_argument('--resume', action='store_true', help='ä»æ£€æŸ¥ç‚¹æ¢å¤')

    args = parser.parse_args()

    processor = BatchProcessor(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        max_evaluators=args.max_evaluators,
        use_enhanced=args.enhanced
    )

    if args.resume:
        processor.load_checkpoint()

    results = processor.run()

    if results:
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ! å…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {processor.results_file}")
        print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Š: {processor.summary_file}")
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
        sys.exit(1)


if __name__ == "__main__":
    main()