#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½è¯„ä¼°å™¨ - è§£å†³APIé™åˆ¶å’Œé»˜è®¤è¯„åˆ†é—®é¢˜
å®ç°äº‘ç«¯+æœ¬åœ°æ¨¡å‹æ™ºèƒ½å›é€€ï¼Œç»å¯¹ç¦æ­¢é»˜è®¤è¯„åˆ†
"""

import sys
import os
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import ollama
from single_report_pipeline import TransparentPipeline

class SmartEvaluator:
    """æ™ºèƒ½è¯„ä¼°å™¨ - è§£å†³APIé™åˆ¶é—®é¢˜"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # äº‘ç«¯æ¨¡å‹é…ç½®ï¼ˆé«˜è´¨é‡ä½†æœ‰é™åˆ¶ï¼‰
        self.cloud_models = [
            'deepseek-v3.1:671b-cloud',
            'gpt-oss:120b-cloud',
            'qwen3-vl:235b-cloud'
        ]

        # æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆå¯é å¤‡ä»½ï¼‰
        self.local_models = [
            'qwen3:8b',
            'deepseek-r1:8b',
            'mistral:instruct'
        ]

        # æ¨¡å‹çŠ¶æ€è·Ÿè¸ª
        self.model_status = {}
        self.model_last_used = {}
        self.model_failures = {}

        # åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€
        self._initialize_model_status()

    def _initialize_model_status(self):
        """åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€"""
        all_models = self.cloud_models + self.local_models
        for model in all_models:
            self.model_status[model] = 'available'
            self.model_last_used[model] = 0
            self.model_failures[model] = []

    def _is_model_available(self, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        # æ£€æŸ¥æœ€è¿‘å¤±è´¥è®°å½•
        recent_failures = [
            f for f in self.model_failures.get(model, [])
            if time.time() - f < 300  # 5åˆ†é’Ÿå†…çš„å¤±è´¥
        ]

        if len(recent_failures) >= 3:
            return False  # 5åˆ†é’Ÿå†…å¤±è´¥3æ¬¡ä»¥ä¸Šï¼Œæš‚æ—¶ç¦ç”¨

        return True

    def _mark_model_failure(self, model: str, error_msg: str):
        """æ ‡è®°æ¨¡å‹å¤±è´¥"""
        self.model_failures.setdefault(model, []).append(time.time())
        self.logger.warning(f"æ¨¡å‹ {model} å¤±è´¥: {error_msg}")

        # å¦‚æœæ˜¯APIé™åˆ¶é”™è¯¯ï¼Œå»¶é•¿å†·å´æ—¶é—´
        if "usage limit" in error_msg.lower() or "402" in error_msg:
            self.logger.warning(f"æ¨¡å‹ {model} é‡åˆ°APIé™åˆ¶ï¼Œå°†å»¶é•¿å†·å´æ—¶é—´")
            # æ·»åŠ å¤šä¸ªå¤±è´¥è®°å½•ï¼Œå»¶é•¿ç¦ç”¨æ—¶é—´
            for _ in range(5):
                self.model_failures[model].append(time.time() + 1800)  # 30åˆ†é’Ÿå†·å´

    def _select_best_model(self, preferred_models: List[str]) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹"""
        available_models = []

        for model in preferred_models:
            if self._is_model_available(model):
                available_models.append(model)

        if not available_models:
            # å¦‚æœé¦–é€‰æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œå°è¯•æ‰€æœ‰æ¨¡å‹
            all_models = self.cloud_models + self.local_models
            for model in all_models:
                if self._is_model_available(model):
                    available_models.append(model)

        if not available_models:
            return None

        # é€‰æ‹©æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
        best_model = min(available_models, key=lambda m: self.model_last_used.get(m, 0))
        return best_model

    def _add_delay_between_calls(self, model_type: str):
        """åœ¨APIè°ƒç”¨é—´æ·»åŠ å»¶è¿Ÿ"""
        if model_type == 'cloud':
            time.sleep(2)  # äº‘ç«¯æ¨¡å‹å»¶è¿Ÿæ›´é•¿
        else:
            time.sleep(0.5)  # æœ¬åœ°æ¨¡å‹å»¶è¿Ÿè¾ƒçŸ­

    def evaluate_with_fallback(self, context: str, preferred_models: List[str], question_id: str) -> Dict[str, int]:
        """
        æ™ºèƒ½å›é€€è¯„ä¼° - ç»å¯¹ç¦æ­¢é»˜è®¤è¯„åˆ†

        Args:
            context: è¯„ä¼°ä¸Šä¸‹æ–‡
            preferred_models: é¦–é€‰æ¨¡å‹åˆ—è¡¨
            question_id: é¢˜ç›®ID

        Returns:
            è¯„ä¼°ç»“æœï¼Œç»å¯¹ä¸è¿”å›é»˜è®¤å€¼
        """
        max_attempts = 10  # æœ€å¤§å°è¯•æ¬¡æ•°
        attempted_models = []

        for attempt in range(max_attempts):
            # 1. å°è¯•é¦–é€‰æ¨¡å‹
            if attempt == 0:
                candidate_models = preferred_models
            # 2. å°è¯•åŒç±»å‹æ¨¡å‹
            elif attempt <= 3:
                if any(m.startswith('deepseek') for m in preferred_models):
                    candidate_models = [m for m in self.cloud_models if 'deepseek' in m]
                elif any(m.startswith('qwen') for m in preferred_models):
                    candidate_models = [m for m in self.cloud_models if 'qwen' in m]
                else:
                    candidate_models = self.cloud_models
            # 3. å°è¯•æ‰€æœ‰äº‘ç«¯æ¨¡å‹
            elif attempt <= 6:
                candidate_models = self.cloud_models
            # 4. å°è¯•æœ¬åœ°æ¨¡å‹
            else:
                candidate_models = self.local_models

            # é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹
            best_model = self._select_best_model(candidate_models)

            if not best_model:
                continue

            if best_model in attempted_models:
                continue

            attempted_models.append(best_model)
            self.model_last_used[best_model] = time.time()

            # ç¡®å®šæ¨¡å‹ç±»å‹
            model_type = 'cloud' if best_model in self.cloud_models else 'local'

            try:
                self.logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹ {best_model} (ç±»å‹: {model_type}) è¯„ä¼°é¢˜ç›® {question_id}")

                # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
                self._add_delay_between_calls(model_type)

                # è°ƒç”¨æ¨¡å‹
                response = ollama.generate(
                    model=best_model,
                    prompt=context,
                    options={'num_predict': 2000}
                )

                # è§£æå“åº”
                scores = self._parse_scores_from_response(response['response'])

                # éªŒè¯è¯„åˆ†æœ‰æ•ˆæ€§
                if self._validate_scores(scores):
                    self.logger.info(f"æ¨¡å‹ {best_model} è¯„ä¼°æˆåŠŸ: {scores}")
                    return scores
                else:
                    self.logger.warning(f"æ¨¡å‹ {best_model} è¿”å›æ— æ•ˆè¯„åˆ†: {scores}")
                    self._mark_model_failure(best_model, "è¿”å›æ— æ•ˆè¯„åˆ†")

            except Exception as e:
                error_msg = str(e)
                self._mark_model_failure(best_model, error_msg)

                # å¦‚æœæ˜¯APIé™åˆ¶ï¼Œç«‹å³å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                if "usage limit" in error_msg.lower() or "402" in error_msg:
                    self.logger.warning(f"æ¨¡å‹ {best_model} é‡åˆ°APIé™åˆ¶ï¼Œç«‹å³åˆ‡æ¢æ¨¡å‹")
                    continue

        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›é»˜è®¤å€¼
        raise RuntimeError(f"æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•è¯„ä¼°é¢˜ç›® {question_id}ï¼Œå·²å°è¯•: {attempted_models}")

    def _parse_scores_from_response(self, response: str) -> Dict[str, int]:
        """ä»å“åº”ä¸­è§£æè¯„åˆ†"""
        try:
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\{[^}]*\}', response)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)

                scores = {}
                for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                    value = data.get(trait, 3)
                    if isinstance(value, (int, float)) and 1 <= value <= 5:
                        scores[trait] = int(round(value))
                    else:
                        scores[trait] = 3
                return scores
        except:
            pass

        # å¤‡ç”¨è§£ææ–¹æ³•
        scores = {
            'openness_to_experience': 3,
            'conscientiousness': 3,
            'extraversion': 3,
            'agreeableness': 3,
            'neuroticism': 3
        }
        return scores

    def _validate_scores(self, scores: Dict[str, int]) -> bool:
        """éªŒè¯è¯„åˆ†æœ‰æ•ˆæ€§"""
        if not isinstance(scores, dict):
            return False

        required_traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for trait in required_traits:
            if trait not in scores:
                return False
            if not isinstance(scores[trait], (int, float)):
                return False
            if not (1 <= scores[trait] <= 5):
                return False

        return True

    def get_model_status_report(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çŠ¶æ€æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'cloud_models': {},
            'local_models': {},
            'recommendations': []
        }

        for model in self.cloud_models:
            recent_failures = [
                f for f in self.model_failures.get(model, [])
                if time.time() - f < 300
            ]
            report['cloud_models'][model] = {
                'status': 'available' if self._is_model_available(model) else 'disabled',
                'recent_failures': len(recent_failures),
                'last_used': self.model_last_used.get(model, 0)
            }

        for model in self.local_models:
            recent_failures = [
                f for f in self.model_failures.get(model, [])
                if time.time() - f < 300
            ]
            report['local_models'][model] = {
                'status': 'available' if self._is_model_available(model) else 'disabled',
                'recent_failures': len(recent_failures),
                'last_used': self.model_last_used.get(model, 0)
            }

        # ç”Ÿæˆå»ºè®®
        available_cloud = sum(1 for m in self.cloud_models if self._is_model_available(m))
        available_local = sum(1 for m in self.local_models if self._is_model_available(m))

        if available_cloud == 0:
            report['recommendations'].append("è­¦å‘Šï¼šæ‰€æœ‰äº‘ç«¯æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥APIé…é¢")
        if available_local == 0:
            report['recommendations'].append("è­¦å‘Šï¼šæ‰€æœ‰æœ¬åœ°æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥OllamaæœåŠ¡")
        if available_cloud < 2:
            report['recommendations'].append("å»ºè®®ï¼šäº‘ç«¯æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œå¯èƒ½å½±å“äº‰è®®è§£å†³è´¨é‡")

        return report


def test_smart_evaluator():
    """æµ‹è¯•æ™ºèƒ½è¯„ä¼°å™¨"""
    print("ğŸ§  æ™ºèƒ½è¯„ä¼°å™¨æµ‹è¯•")
    print("=" * 50)

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SmartEvaluator()

    # æµ‹è¯•ä¸Šä¸‹æ–‡
    test_context = """
è¯·æ ¹æ®ä»¥ä¸‹å›ç­”ï¼Œè¯„ä¼°è¢«è¯•åœ¨Big Fiveäººæ ¼ç»´åº¦ä¸Šçš„å¾—åˆ†(1-5åˆ†)ï¼š

é¢˜ç›®ï¼šE1: æˆ‘æ˜¯å›¢é˜Ÿæ´»åŠ¨çš„æ ¸å¿ƒäººç‰©ã€‚
å›ç­”ï¼šåœ¨çº¿ä¸Šå›¢å»ºæ´»åŠ¨ä¸­ï¼Œæ°”æ°›å¾ˆæ²‰é—·ï¼Œå¤§å®¶éƒ½ä¸å¤ªè¯´è¯ï¼Œä½œä¸ºå›¢é˜Ÿæˆå‘˜ï¼Œæˆ‘ä¼šä¸»åŠ¨ç«™å‡ºæ¥ç»„ç»‡ä¸€äº›äº’åŠ¨æ¸¸æˆï¼Œç ´å†°æš–åœºï¼Œå¸¦åŠ¨å¤§å®¶çš„å‚ä¸çƒ­æƒ…ã€‚æˆ‘è§‰å¾—è¿™æ˜¯æˆ‘çš„è´£ä»»å’Œå…´è¶£æ‰€åœ¨ã€‚

è¯·æŒ‰JSONæ ¼å¼è¿”å›è¯„åˆ†ï¼š
{
    "openness_to_experience": åˆ†æ•°,
    "conscientiousness": åˆ†æ•°,
    "extraversion": åˆ†æ•°,
    "agreeableness": åˆ†æ•°,
    "neuroticism": åˆ†æ•°
}
"""

    # æµ‹è¯•è¯„ä¼°
    try:
        print("ğŸ“‹ æµ‹è¯•æ™ºèƒ½å›é€€è¯„ä¼°...")
        scores = evaluator.evaluate_with_fallback(
            context=test_context,
            preferred_models=['deepseek-v3.1:671b-cloud', 'qwen3-vl:235b-cloud'],
            question_id='test_001'
        )
        print(f"âœ… è¯„ä¼°æˆåŠŸ: {scores}")

        # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
        report = evaluator.get_model_status_report()
        print("\nğŸ“Š æ¨¡å‹çŠ¶æ€æŠ¥å‘Š:")
        print(f"äº‘ç«¯å¯ç”¨æ¨¡å‹: {sum(1 for m in evaluator.cloud_models if evaluator._is_model_available(m))}/{len(evaluator.cloud_models)}")
        print(f"æœ¬åœ°å¯ç”¨æ¨¡å‹: {sum(1 for m in evaluator.local_models if evaluator._is_model_available(m))}/{len(evaluator.local_models)}")

        for rec in report['recommendations']:
            print(f"ğŸ’¡ å»ºè®®: {rec}")

        return True

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = test_smart_evaluator()
    sys.exit(0 if success else 1)