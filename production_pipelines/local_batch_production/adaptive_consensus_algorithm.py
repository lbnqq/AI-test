#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é€‚åº”æ€§å…±è¯†ç®—æ³•
åŸºäºç”¨æˆ·éœ€æ±‚çš„åŠ¨æ€è¯„ä¼°å™¨å…±è¯†ç®—æ³•
å½“å‡ºç°åˆ†æ­§æ—¶è‡ªåŠ¨å¢åŠ è¯„ä¼°å™¨ç›´åˆ°è¾¾æˆå¯é å…±è¯†
"""

import statistics
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import random


class AdaptiveConsensusAlgorithm:
    """
    æ™ºèƒ½é€‚åº”æ€§å…±è¯†ç®—æ³•

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. åŸºç¡€3ä¸ªè¯„ä¼°å™¨ï¼Œè¯„åˆ†åªèƒ½æ˜¯1, 3, 5
    2. è‡ªåŠ¨æ£€æµ‹åˆ†æ­§ç¨‹åº¦
    3. åŠ¨æ€å¢åŠ è¯„ä¼°å™¨ç›´åˆ°è¾¾æˆå…±è¯†
    4. æ™ºèƒ½åå·®æ£€æµ‹å’Œæ’é™¤
    5. æœ€å¤šæ‰©å±•åˆ°7ä¸ªè¯„ä¼°å™¨
    """

    def __init__(self):
        # ç®—æ³•å‚æ•°
        self.initial_evaluators = 3
        self.max_evaluators = 7
        self.allowed_scores = [1, 3, 5]  # åªå…è®¸å¥‡æ•°è¯„åˆ†
        self.consensus_threshold = 2.0    # æœ€å¤§å…è®¸å·®å¼‚
        self.bias_detection_threshold = 1.5  # åå·®æ£€æµ‹é˜ˆå€¼

        # è¯„ä¼°å™¨æ± ï¼ˆç”¨äºåŠ¨æ€æ‰©å±•ï¼‰
        self.evaluator_pool = [
            'evaluator_a', 'evaluator_b', 'evaluator_c',
            'evaluator_d', 'evaluator_e', 'evaluator_f', 'evaluator_g'
        ]

    def adaptive_consensus(self, initial_scores: List[int],
                          get_additional_scores: callable) -> Dict[str, Any]:
        """
        è‡ªé€‚åº”å…±è¯†ç®—æ³•ä¸»å…¥å£

        Args:
            initial_scores: åˆå§‹3ä¸ªè¯„ä¼°å™¨çš„è¯„åˆ† [1,3,5]
            get_additional_scores: è·å–é¢å¤–è¯„ä¼°å™¨è¯„åˆ†çš„å‡½æ•°

        Returns:
            å…±è¯†ç»“æœå­—å…¸
        """
        if len(initial_scores) != 3:
            raise ValueError("åˆå§‹è¯„åˆ†å¿…é¡»æ°å¥½åŒ…å«3ä¸ªè¯„ä¼°å™¨çš„è¯„åˆ†")

        # éªŒè¯è¯„åˆ†åˆæ³•æ€§
        if not all(score in self.allowed_scores for score in initial_scores):
            raise ValueError("è¯„åˆ†åªèƒ½æ˜¯1, 3, 5")

        return self._adaptive_consensus_process(initial_scores, get_additional_scores)

    def _adaptive_consensus_process(self, scores: List[int],
                                  get_additional_scores: callable,
                                  round_num: int = 1) -> Dict[str, Any]:
        """é€’å½’å…±è¯†å¤„ç†è¿‡ç¨‹"""

        print(f"ğŸ”„ ç¬¬{round_num}è½®å…±è¯†å¤„ç†ï¼Œå½“å‰è¯„åˆ†: {scores}")

        # è®¡ç®—å½“å‰è¯„åˆ†çš„å·®å¼‚
        max_score, min_score = max(scores), min(scores)
        current_diff = max_score - min_score

        if current_diff == 0:
            # æƒ…å†µ1: å®Œå…¨å…±è¯†
            print(f"âœ… å®Œå…¨å…±è¯†ï¼æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯ {max_score}")
            return self._create_result(scores, max_score, "perfect_consensus", round_num)

        elif current_diff <= 2:
            # æƒ…å†µ2: è½»å¾®åˆ†æ­§ (å·®å¼‚â‰¤2åˆ†)
            return self._handle_minor_disagreement(scores, round_num)

        elif current_diff == 4:
            # æƒ…å†µ3: ä¸¥é‡åˆ†æ­§ (å·®å¼‚=4åˆ†) - ç®€åŒ–å¤„ç†
            return self._handle_major_disagreement_simple(scores, get_additional_scores, round_num)

        else:
            # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸ºè¯„åˆ†åªèƒ½æ˜¯1,3,5ï¼‰
            raise ValueError(f"å¼‚å¸¸å·®å¼‚: {current_diff}")

    def _handle_minor_disagreement(self, scores: List[int], round_num: int) -> Dict[str, Any]:
        """å¤„ç†è½»å¾®åˆ†æ­§ï¼ˆå·®å¼‚â‰¤2åˆ†ï¼‰"""

        score_counts = Counter(scores)

        if len(score_counts) == 2:
            # æœ‰ä¸¤ä¸ªåˆ†æ•°ï¼Œä¸€ä¸ªè½»å¾®ä¸åŒ
            most_common = score_counts.most_common(1)[0][0]
            consensus_score = statistics.mean(scores)

            print(f"âœ… è½»å¾®åˆ†æ­§è¾¾æˆå…±è¯†: {scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
            return self._create_result(scores, consensus_score, "minor_consensus", round_num)

        else:
            # ä¸‰ä¸ªåˆ†æ•°éƒ½ä¸åŒï¼Œä½†å·®å¼‚åœ¨2åˆ†å†…ï¼ˆæ¯”å¦‚[1,3,3]æˆ–[3,5,5]ï¼‰
            consensus_score = statistics.mean(scores)

            print(f"âœ… è½»å¾®åˆ†æ­§è¾¾æˆå…±è¯†: {scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
            return self._create_result(scores, consensus_score, "minor_consensus", round_num)

    def _handle_major_disagreement_simple(self, scores: List[int], get_additional_scores: callable, round_num: int) -> Dict[str, Any]:
        """
        ä¸¥é‡åˆ†æ­§å¤„ç†ï¼ˆå·®å¼‚=4åˆ†ï¼‰
        æ–°è§„åˆ™ï¼šå‰é¢3ä¸ªè¯„ä¼°å™¨åªç®—ä½œ1ä¸ªè¯„åˆ†ï¼Œç„¶åç»§ç»­ä¸‹ä¸€è½®å…±è¯†
        """
        print(f"âš ï¸ ä¸¥é‡åˆ†æ­§å¤„ç†: {scores} (å·®å¼‚=4åˆ†)")

        # å°†å‰é¢3ä¸ªè¯„åˆ†åˆå¹¶ä¸º1ä¸ªè¯„åˆ†
        median_score = statistics.median(scores)

        # åˆ¤æ–­æ˜¯å¦æœ‰çœŸæ­£çš„ä¸­ä½æ•°ï¼ˆå³æœ‰é‡å¤å€¼ï¼‰
        score_counts = Counter(scores)
        has_median = any(count > 1 for count in score_counts.values())

        if has_median:
            # æœ‰ä¸­ä½æ•°ï¼ˆæœ‰é‡å¤å€¼ï¼‰
            consolidated_score = median_score
            print(f"ğŸ”„ 3ä¸ªè¯„ä¼°å™¨åˆå¹¶ä¸º1ä¸ªè¯„åˆ†: {scores} -> {consolidated_score} (å–ä¸­ä½æ•°)")
        else:
            # æ²¡æœ‰ä¸­ä½æ•°ï¼ˆä¸‰ä¸ªåˆ†æ•°éƒ½ä¸åŒï¼‰ï¼Œå–å¹³å‡æ•°
            consolidated_score = statistics.mean(scores)
            print(f"ğŸ”„ 3ä¸ªè¯„ä¼°å™¨åˆå¹¶ä¸º1ä¸ªè¯„åˆ†: {scores} -> {consolidated_score:.1f} (å–å¹³å‡æ•°)")

        # è·å–æ–°çš„è¯„ä¼°å™¨è¯„åˆ†ï¼ˆè‡³å°‘éœ€è¦2ä¸ªæ‰èƒ½è¿›è¡Œå…±è¯†ï¼‰
        new_scores = get_additional_scores(2)
        print(f"ğŸ”„ æ–°å¢è¯„ä¼°å™¨è¯„åˆ†: {new_scores}")

        # å°†åˆå¹¶çš„è¯„åˆ†ä¸æ–°è¯„åˆ†ä¸€èµ·è¿›è¡Œä¸‹ä¸€è½®å…±è¯†
        all_scores = [int(consolidated_score)] + new_scores
        print(f"ğŸ”„ è¿›å…¥ä¸‹ä¸€è½®å…±è¯†ï¼Œå½“å‰è¯„åˆ†: {all_scores}")

        # é€’å½’å¤„ç†ä¸‹ä¸€è½®å…±è¯†
        return self._adaptive_consensus_process(all_scores, get_additional_scores, round_num + 1)

    def _handle_major_disagreement(self, scores: List[int],
                                 get_additional_scores: callable,
                                 round_num: int) -> Dict[str, Any]:
        """å¤„ç†ä¸¥é‡åˆ†æ­§ï¼ˆå·®å¼‚=4åˆ†ï¼‰"""

        score_counts = Counter(scores)

        if len(score_counts) == 2:
            # æœ‰ä¸¤ä¸ªç›¸åŒåˆ†æ•°ï¼Œä¸€ä¸ªå·®å¼‚å¾ˆå¤§
            common_score, uncommon_score = score_counts.most_common(2)
            common_score = common_score[0]
            uncommon_score = uncommon_score[0]

            print(f"âš ï¸ ä¸¥é‡åˆ†æ­§æ£€æµ‹åˆ°: {scores} (å…±è¯†: {common_score}, å¼‚å¸¸: {uncommon_score})")

            # åºŸå¼ƒå·®å¼‚å¤§çš„åˆ†æ•°ï¼Œæ–°å¢2ä¸ªè¯„ä¼°å™¨
            new_scores = get_additional_scores(2)
            extended_scores = [common_score] + new_scores

            print(f"ğŸ”„ æ–°å¢2ä¸ªè¯„ä¼°å™¨è¯„åˆ†: {new_scores}")

            # æ£€æŸ¥æ–°çš„ä¸€è‡´æ€§
            new_max, new_min = max(extended_scores), min(extended_scores)
            new_diff = new_max - new_min

            if new_diff <= 2:
                # è¾¾æˆå…±è¯†ï¼Œå–4ä¸ªåˆ†æ•°çš„å¹³å‡
                consensus_score = statistics.mean(extended_scores)
                print(f"âœ… æ‰©å±•åè¾¾æˆå…±è¯†: {extended_scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
                return self._create_result(extended_scores, consensus_score, "extended_consensus", round_num)

            else:
                # ä»ç„¶åˆ†æ­§å¾ˆå¤§ï¼Œè¿›ä¸€æ­¥å¤„ç†
                return self._handle_still_divided(extended_scores, uncommon_score,
                                                get_additional_scores, round_num)

        else:
            # ä¸‰ä¸ªåˆ†æ•°éƒ½ä¸åŒï¼ˆ1,3,5ï¼‰ï¼Œè¿™ç¡®å®å·®å¼‚4åˆ†
            # è¿™æ˜¯æœ€å¤§åˆ†æ­§æƒ…å†µï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            print(f"âš ï¸ æœ€å¤§åˆ†æ­§æ£€æµ‹åˆ°: {scores} (ä¸‰ä¸ªåˆ†æ•°éƒ½ä¸åŒ)")
            # ç›´æ¥è¿›å…¥æ‰©å±•é˜¶æ®µ
            new_scores = get_additional_scores(2)
            extended_scores = scores + new_scores

            print(f"ğŸ”„ æ–°å¢2ä¸ªè¯„ä¼°å™¨è¯„åˆ†: {new_scores}")

            # å»æ‰åå·®æœ€å¤§çš„ä¸¤ä¸ªåˆ†æ•°
            bias_removed_scores = self._remove_max_bias(extended_scores, 2)
            consensus_score = statistics.mean(bias_removed_scores)

            print(f"âœ… æœ€å¤§åˆ†æ­§è§£å†³: {bias_removed_scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
            return self._create_result(bias_removed_scores, consensus_score, "max_divergence_consensus", round_num)

    def _handle_still_divided(self, current_scores: List[int],
                            discarded_score: int,
                            get_additional_scores: callable,
                            round_num: int) -> Dict[str, Any]:
        """å¤„ç†æ‰©å±•åä»ç„¶åˆ†æ­§çš„æƒ…å†µ"""

        # ä¸åŸæ¥åºŸå¼ƒçš„åˆ†æ•°æ¯”è¾ƒ
        all_scores = current_scores + [discarded_score]
        current_mean = statistics.mean(current_scores)

        if abs(discarded_score - current_mean) <= 2:
            # åºŸå¼ƒçš„åˆ†æ•°å…¶å®å¹¶ä¸å¤ªåå·®ï¼Œé‡æ–°çº³å…¥è€ƒè™‘
            print(f"ğŸ”„ é‡æ–°è€ƒè™‘åºŸå¼ƒåˆ†æ•° {discarded_score}ï¼Œå½“å‰5ä¸ªè¯„åˆ†: {all_scores}")

            # å»æ‰åå·®æœ€å¤§çš„ä¸€ä¸ªåˆ†æ•°
            bias_removed_scores = self._remove_max_bias(all_scores, 1)
            consensus_score = statistics.mean(bias_removed_scores)

            print(f"âœ… å»é™¤æœ€å¤§åå·®åè¾¾æˆå…±è¯†: {bias_removed_scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
            return self._create_result(bias_removed_scores, consensus_score, "bias_removed_consensus", round_num)

        else:
            # åºŸå¼ƒçš„åˆ†æ•°ç¡®å®åå·®å¾ˆå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥æ‰©å±•
            if len(current_scores) < 7:
                # ç»§ç»­å¢åŠ 2ä¸ªè¯„ä¼°å™¨
                new_scores = get_additional_scores(2)
                extended_scores = current_scores + new_scores

                print(f"ğŸ”„ è¿›ä¸€æ­¥æ‰©å±•ï¼Œæ–°å¢2ä¸ªè¯„ä¼°å™¨: {new_scores}")
                print(f"ğŸ“Š å½“å‰7ä¸ªè¯„åˆ†: {extended_scores}")

                # å»æ‰æœ€å¤§åå·®çš„ä¸¤ä¸ªåˆ†æ•°
                bias_removed_scores = self._remove_max_bias(extended_scores, 2)
                consensus_score = statistics.mean(bias_removed_scores)

                print(f"âœ… æœ€ç»ˆå…±è¯†: {bias_removed_scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
                return self._create_result(bias_removed_scores, consensus_score, "final_consensus", round_num)
            else:
                # å·²è¾¾åˆ°7ä¸ªè¯„ä¼°å™¨çš„ä¸Šé™
                bias_removed_scores = self._remove_max_bias(current_scores, 1)
                consensus_score = statistics.mean(bias_removed_scores)

                print(f"âš ï¸ è¾¾åˆ°è¯„ä¼°å™¨ä¸Šé™ï¼Œå¼ºåˆ¶å…±è¯†: {bias_removed_scores} -> å¹³å‡åˆ† {consensus_score:.1f}")
                return self._create_result(bias_removed_scores, consensus_score, "forced_consensus", round_num)

    def _remove_max_bias(self, scores: List[int], remove_count: int) -> List[int]:
        """ç§»é™¤åå·®æœ€å¤§çš„åˆ†æ•°"""

        if len(scores) <= remove_count:
            raise ValueError(f"æ— æ³•ä» {len(scores)} ä¸ªè¯„åˆ†ä¸­ç§»é™¤ {remove_count} ä¸ª")

        median_score = statistics.median(scores)

        # è®¡ç®—æ¯ä¸ªåˆ†æ•°ä¸ä¸­ä½æ•°çš„åå·®ï¼ˆæ›´ç§‘å­¦ï¼‰
        biases = [abs(score - median_score) for score in scores]

        # æŒ‰åå·®ä»å°åˆ°å¤§æ’åºï¼Œä¿ç•™åå·®è¾ƒå°çš„
        scored_scores = list(zip(scores, biases))
        scored_scores.sort(key=lambda x: x[1])

        # ç§»é™¤åå·®æœ€å¤§çš„å‡ ä¸ªåˆ†æ•°
        kept_scores = [score for score, _ in scored_scores[:-remove_count]]

        print(f"ğŸ¯ åå·®åˆ†æ: ä¸­ä½æ•°={median_score:.2f}, ç§»é™¤åå·®æœ€å¤§çš„{remove_count}ä¸ªåˆ†æ•°")
        print(f"   åå·®è¯¦æƒ…: {[(f'{s}(åå·®{b:.2f})') for s, b in scored_scores]}")

        return kept_scores

    def _create_result(self, scores: List[int], consensus_score: float,
                      method: str, round_num: int) -> Dict[str, Any]:
        """åˆ›å»ºå…±è¯†ç»“æœ"""

        return {
            'consensus_score': round(consensus_score, 2),
            'final_scores': scores,
            'evaluator_count': len(scores),
            'consensus_method': method,
            'processing_rounds': round_num,
            'score_distribution': dict(Counter(scores)),
            'quality_metrics': self._calculate_quality_metrics(scores)
        }

    def _calculate_quality_metrics(self, scores: List[int]) -> Dict[str, Any]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""

        if len(scores) < 2:
            return {'consensus_strength': 1.0, 'agreement_level': 'perfect'}

        # å…±è¯†å¼ºåº¦ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
        std_dev = statistics.stdev(scores) if len(scores) > 1 else 0
        consensus_strength = max(0.0, 1.0 - (std_dev / 2.0))

        # åŒæ„ç¨‹åº¦
        max_count = max(Counter(scores).values())
        agreement_ratio = max_count / len(scores)

        if agreement_ratio >= 0.8:
            agreement_level = 'high'
        elif agreement_ratio >= 0.6:
            agreement_level = 'medium'
        else:
            agreement_level = 'low'

        return {
            'consensus_strength': round(consensus_strength, 3),
            'agreement_level': agreement_level,
            'agreement_ratio': round(agreement_ratio, 3),
            'evaluator_diversity': len(set(scores))
        }


def demo_adaptive_consensus():
    """æ¼”ç¤ºè‡ªé€‚åº”å…±è¯†ç®—æ³•"""

    print("ğŸ§  æ™ºèƒ½é€‚åº”æ€§å…±è¯†ç®—æ³•æ¼”ç¤º")
    print("=" * 60)

    algorithm = AdaptiveConsensusAlgorithm()

    # æ¨¡æ‹Ÿè·å–é¢å¤–è¯„ä¼°å™¨è¯„åˆ†çš„å‡½æ•°
    def mock_additional_scores(count: int) -> List[int]:
        """æ¨¡æ‹Ÿé¢å¤–è¯„ä¼°å™¨è¯„åˆ†"""
        # ä¸ºäº†æ¼”ç¤ºï¼Œéšæœºç”Ÿæˆè¯„åˆ†ï¼Œä½†å€¾å‘äºå·²æœ‰è¯„åˆ†çš„ä¼—æ•°
        available_scores = [1, 3, 5]
        weights = [0.2, 0.6, 0.2]  # å€¾å‘äºç»™3åˆ†
        return random.choices(available_scores, weights=weights, k=count)

    # æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            'name': 'å®Œå…¨å…±è¯†',
            'scores': [3, 3, 3]
        },
        {
            'name': 'è½»å¾®åˆ†æ­§',
            'scores': [3, 3, 5]
        },
        {
            'name': 'ä¸¥é‡åˆ†æ­§',
            'scores': [1, 3, 3]
        },
        {
            'name': 'æç«¯åˆ†æ­§',
            'scores': [1, 1, 5]
        }
    ]

    for scenario in test_scenarios:
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: {scenario['name']}")
        print(f"åˆå§‹è¯„åˆ†: {scenario['scores']}")
        print("-" * 40)

        result = algorithm.adaptive_consensus(scenario['scores'], mock_additional_scores)

        print(f"\nğŸ“‹ å…±è¯†ç»“æœ:")
        print(f"  å…±è¯†è¯„åˆ†: {result['consensus_score']}")
        print(f"  æœ€ç»ˆè¯„åˆ†: {result['final_scores']}")
        print(f"  è¯„ä¼°å™¨æ•°é‡: {result['evaluator_count']}")
        print(f"  å…±è¯†æ–¹æ³•: {result['consensus_method']}")
        print(f"  å¤„ç†è½®æ•°: {result['processing_rounds']}")
        print(f"  è´¨é‡æŒ‡æ ‡: {result['quality_metrics']}")
        print()


if __name__ == "__main__":
    demo_adaptive_consensus()